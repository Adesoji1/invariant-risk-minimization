{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "invariant-risk-minimization-colored-mnist.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reiinakano/invariant-risk-minimization/blob/master/invariant_risk_minimization_colored_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLDC06POu-Z5",
        "colab_type": "text"
      },
      "source": [
        "# Invariant Risk Minimization\n",
        "\n",
        "This is an attempt to reproduce the \"Colored MNIST\" experiments from the\n",
        "paper [Invariant Risk Minimization](https://arxiv.org/abs/1907.02893)\n",
        "by Arjovsky, et. al."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sopHPgEhu4Jo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import grad\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "import torchvision.datasets.utils as dataset_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_u5rBUnvdxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkTMK-oJveGg",
        "colab_type": "text"
      },
      "source": [
        "## Prepare the colored MNIST dataset\n",
        "\n",
        "We define three environments (two training, one test) by randomly splitting the MNIST dataset in thirds and transforming each example as follows:\n",
        "1. Assign a binary label y to the image based on the digit: y = 0 for digits 0-4\n",
        "and y = 1 for digits 5-9.\n",
        "2. Flip the label with 25% probability.\n",
        "3. Color the image either red or green according to its (possibly flipped) label.\n",
        "4. Flip the color with a probability e that depends on the environment: 20% in\n",
        "the first training environment, 10% in the second training environment, and\n",
        "90% in the test environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knP-xNzavgAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def color_grayscale_arr(arr, red=True):\n",
        "  \"\"\"Converts grayscale image to either red or green\"\"\"\n",
        "  assert arr.ndim == 2\n",
        "  dtype = arr.dtype\n",
        "  h, w = arr.shape\n",
        "  arr = np.reshape(arr, [h, w, 1])\n",
        "  if red:\n",
        "    arr = np.concatenate([arr,\n",
        "                          np.zeros((h, w, 2), dtype=dtype)], axis=2)\n",
        "  else:\n",
        "    arr = np.concatenate([np.zeros((h, w, 1), dtype=dtype),\n",
        "                          arr,\n",
        "                          np.zeros((h, w, 1), dtype=dtype)], axis=2)\n",
        "  return arr\n",
        "\n",
        "\n",
        "class ColoredMNIST(datasets.VisionDataset):\n",
        "  \"\"\"\n",
        "  Colored MNIST dataset for testing IRM. Prepared using procedure from https://arxiv.org/pdf/1907.02893.pdf\n",
        "\n",
        "  Args:\n",
        "    root (string): Root directory of dataset where ``ColoredMNIST/*.pt`` will exist.\n",
        "    env (string): Which environment to load. Must be 1 of 'train1', 'train2', 'test', or 'all_train'.\n",
        "    transform (callable, optional): A function/transform that  takes in an PIL image\n",
        "      and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "    target_transform (callable, optional): A function/transform that takes in the\n",
        "      target and transforms it.\n",
        "  \"\"\"\n",
        "  def __init__(self, root='./data', env='train1', transform=None, target_transform=None):\n",
        "    super(ColoredMNIST, self).__init__(root, transform=transform,\n",
        "                                target_transform=target_transform)\n",
        "\n",
        "    self.prepare_colored_mnist()\n",
        "    if env in ['train1', 'train2', 'test']:\n",
        "      self.data_label_tuples = torch.load(os.path.join(self.root, 'ColoredMNIST', env) + '.pt')\n",
        "    elif env == 'all_train':\n",
        "      self.data_label_tuples = torch.load(os.path.join(self.root, 'ColoredMNIST', 'train1.pt')) + \\\n",
        "                               torch.load(os.path.join(self.root, 'ColoredMNIST', 'train2.pt'))\n",
        "    else:\n",
        "      raise RuntimeError(f'{env} env unknown. Valid envs are train1, train2, test, and all_train')\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        index (int): Index\n",
        "\n",
        "    Returns:\n",
        "        tuple: (image, target) where target is index of the target class.\n",
        "    \"\"\"\n",
        "    img, target = self.data_label_tuples[index]\n",
        "\n",
        "    if self.transform is not None:\n",
        "      img = self.transform(img)\n",
        "\n",
        "    if self.target_transform is not None:\n",
        "      target = self.target_transform(target)\n",
        "\n",
        "    return img, target\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data_label_tuples)\n",
        "\n",
        "  def prepare_colored_mnist(self):\n",
        "    colored_mnist_dir = os.path.join(self.root, 'ColoredMNIST')\n",
        "    if os.path.exists(os.path.join(colored_mnist_dir, 'train1.pt')) \\\n",
        "        and os.path.exists(os.path.join(colored_mnist_dir, 'train2.pt')) \\\n",
        "        and os.path.exists(os.path.join(colored_mnist_dir, 'test.pt')):\n",
        "      print('Colored MNIST dataset already exists')\n",
        "      return\n",
        "\n",
        "    print('Preparing Colored MNIST')\n",
        "    train_mnist = datasets.mnist.MNIST(self.root, train=True, download=True)\n",
        "\n",
        "    train1_set = []\n",
        "    train2_set = []\n",
        "    test_set = []\n",
        "    for idx, (im, label) in enumerate(train_mnist):\n",
        "      if idx % 10000 == 0:\n",
        "        print(f'Converting image {idx}/{len(train_mnist)}')\n",
        "      im_array = np.array(im)\n",
        "\n",
        "      # Assign a binary label y to the image based on the digit\n",
        "      binary_label = 0 if label < 5 else 1\n",
        "\n",
        "      # Flip label with 25% probability\n",
        "      if np.random.uniform() < 0.25:\n",
        "        binary_label = binary_label ^ 1\n",
        "\n",
        "      # Color the image either red or green according to its possibly flipped label\n",
        "      color_red = binary_label == 0\n",
        "\n",
        "      # Flip the color with a probability e that depends on the environment\n",
        "      if idx < 20000:\n",
        "        # 20% in the first training environment\n",
        "        if np.random.uniform() < 0.2:\n",
        "          color_red = not color_red\n",
        "      elif idx < 40000:\n",
        "        # 10% in the first training environment\n",
        "        if np.random.uniform() < 0.1:\n",
        "          color_red = not color_red\n",
        "      else:\n",
        "        # 90% in the test environment\n",
        "        if np.random.uniform() < 0.9:\n",
        "          color_red = not color_red\n",
        "\n",
        "      colored_arr = color_grayscale_arr(im_array, red=color_red)\n",
        "\n",
        "      if idx < 20000:\n",
        "        train1_set.append((Image.fromarray(colored_arr), binary_label))\n",
        "      elif idx < 40000:\n",
        "        train2_set.append((Image.fromarray(colored_arr), binary_label))\n",
        "      else:\n",
        "        test_set.append((Image.fromarray(colored_arr), binary_label))\n",
        "\n",
        "      # Debug\n",
        "      # print('original label', type(label), label)\n",
        "      # print('binary label', binary_label)\n",
        "      # print('assigned color', 'red' if color_red else 'green')\n",
        "      # plt.imshow(colored_arr)\n",
        "      # plt.show()\n",
        "      # break\n",
        "\n",
        "    dataset_utils.makedir_exist_ok(colored_mnist_dir)\n",
        "    torch.save(train1_set, os.path.join(colored_mnist_dir, 'train1.pt'))\n",
        "    torch.save(train2_set, os.path.join(colored_mnist_dir, 'train2.pt'))\n",
        "    torch.save(test_set, os.path.join(colored_mnist_dir, 'test.pt'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwUoQZCyvs6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jleIZ9vNv5rV",
        "colab_type": "text"
      },
      "source": [
        "## Define neural network\n",
        "\n",
        "The paper uses an MLP but a Convnet works fine too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hYJRewnv80x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.fc1 = nn.Linear(3 * 28 * 28, 512)\n",
        "    self.fc2 = nn.Linear(512, 512)\n",
        "    self.fc3 = nn.Linear(512, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 3 * 28 * 28)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    logits = self.fc3(x).flatten()\n",
        "    return logits\n",
        "\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 20, 5, 1)\n",
        "    self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "    self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
        "    self.fc2 = nn.Linear(500, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = F.max_pool2d(x, 2, 2)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = F.max_pool2d(x, 2, 2)\n",
        "    x = x.view(-1, 4 * 4 * 50)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    logits = self.fc2(x).flatten()\n",
        "    return logits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj5Q6UTlwGM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il3cATYxwIyP",
        "colab_type": "text"
      },
      "source": [
        "## Test ERM as a baseline\n",
        "\n",
        "Using ERM as a baseline, we expect to train a neural network that uses color instead of the actual digit to classify, completely failing on the test set when the colors are switched."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4qZtXx_weBb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3913ba6e-c9e0-444f-92fd-2219f758f96e"
      },
      "source": [
        "def test_model(model, device, test_loader):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(device), target.to(device).float()\n",
        "      output = model(data)\n",
        "      test_loss += F.binary_cross_entropy_with_logits(output, target, reduction='sum').item()  # sum up batch loss\n",
        "      pred = torch.where(torch.gt(output, torch.Tensor([0.0]).to(device)),\n",
        "                         torch.Tensor([1.0]).to(device),\n",
        "                         torch.Tensor([0.0]).to(device))  # get the index of the max log-probability\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "\n",
        "  print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "\n",
        "def erm_train(model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = data.to(device), target.to(device).float()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.binary_cross_entropy_with_logits(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 10 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "               100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def train_and_test_erm():\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "  kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "  all_train_loader = torch.utils.data.DataLoader(\n",
        "    ColoredMNIST(root='./data', env='all_train',\n",
        "                 transform=transforms.Compose([\n",
        "                     transforms.ToTensor(),\n",
        "                     transforms.Normalize((0.1307, 0.1307, 0.), (0.3081, 0.3081, 0.3081))\n",
        "                   ])),\n",
        "    batch_size=64, shuffle=True, **kwargs)\n",
        "\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "    ColoredMNIST(root='./data', env='test', transform=transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.1307, 0.1307, 0.), (0.3081, 0.3081, 0.3081))\n",
        "    ])),\n",
        "    batch_size=1000, shuffle=True, **kwargs)\n",
        "\n",
        "  model = ConvNet().to(device)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "  for epoch in range(1, 3):\n",
        "    erm_train(model, device, all_train_loader, optimizer, epoch)\n",
        "    print('testing on train set')\n",
        "    test_model(model, device, all_train_loader)\n",
        "    print('testing on test set')\n",
        "    test_model(model, device, test_loader)\n",
        "\n",
        "train_and_test_erm()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Preparing Colored MNIST\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:07, 1315545.27it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 58139.90it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:01, 965580.39it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 21552.74it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n",
            "Converting image 0/60000\n",
            "Converting image 10000/60000\n",
            "Converting image 20000/60000\n",
            "Converting image 30000/60000\n",
            "Converting image 40000/60000\n",
            "Converting image 50000/60000\n",
            "Colored MNIST dataset already exists\n",
            "Train Epoch: 1 [0/40000 (0%)]\tLoss: 0.692368\n",
            "Train Epoch: 1 [640/40000 (2%)]\tLoss: 0.625929\n",
            "Train Epoch: 1 [1280/40000 (3%)]\tLoss: 0.604655\n",
            "Train Epoch: 1 [1920/40000 (5%)]\tLoss: 0.530048\n",
            "Train Epoch: 1 [2560/40000 (6%)]\tLoss: 0.479988\n",
            "Train Epoch: 1 [3200/40000 (8%)]\tLoss: 0.452668\n",
            "Train Epoch: 1 [3840/40000 (10%)]\tLoss: 0.422417\n",
            "Train Epoch: 1 [4480/40000 (11%)]\tLoss: 0.553012\n",
            "Train Epoch: 1 [5120/40000 (13%)]\tLoss: 0.166266\n",
            "Train Epoch: 1 [5760/40000 (14%)]\tLoss: 0.279148\n",
            "Train Epoch: 1 [6400/40000 (16%)]\tLoss: 0.498339\n",
            "Train Epoch: 1 [7040/40000 (18%)]\tLoss: 0.520435\n",
            "Train Epoch: 1 [7680/40000 (19%)]\tLoss: 0.438083\n",
            "Train Epoch: 1 [8320/40000 (21%)]\tLoss: 0.295991\n",
            "Train Epoch: 1 [8960/40000 (22%)]\tLoss: 0.399400\n",
            "Train Epoch: 1 [9600/40000 (24%)]\tLoss: 0.540857\n",
            "Train Epoch: 1 [10240/40000 (26%)]\tLoss: 0.299456\n",
            "Train Epoch: 1 [10880/40000 (27%)]\tLoss: 0.506650\n",
            "Train Epoch: 1 [11520/40000 (29%)]\tLoss: 0.454674\n",
            "Train Epoch: 1 [12160/40000 (30%)]\tLoss: 0.578892\n",
            "Train Epoch: 1 [12800/40000 (32%)]\tLoss: 0.402581\n",
            "Train Epoch: 1 [13440/40000 (34%)]\tLoss: 0.352600\n",
            "Train Epoch: 1 [14080/40000 (35%)]\tLoss: 0.443744\n",
            "Train Epoch: 1 [14720/40000 (37%)]\tLoss: 0.334271\n",
            "Train Epoch: 1 [15360/40000 (38%)]\tLoss: 0.435252\n",
            "Train Epoch: 1 [16000/40000 (40%)]\tLoss: 0.251913\n",
            "Train Epoch: 1 [16640/40000 (42%)]\tLoss: 0.334262\n",
            "Train Epoch: 1 [17280/40000 (43%)]\tLoss: 0.304654\n",
            "Train Epoch: 1 [17920/40000 (45%)]\tLoss: 0.449916\n",
            "Train Epoch: 1 [18560/40000 (46%)]\tLoss: 0.351617\n",
            "Train Epoch: 1 [19200/40000 (48%)]\tLoss: 0.397643\n",
            "Train Epoch: 1 [19840/40000 (50%)]\tLoss: 0.354961\n",
            "Train Epoch: 1 [20480/40000 (51%)]\tLoss: 0.477085\n",
            "Train Epoch: 1 [21120/40000 (53%)]\tLoss: 0.364842\n",
            "Train Epoch: 1 [21760/40000 (54%)]\tLoss: 0.470934\n",
            "Train Epoch: 1 [22400/40000 (56%)]\tLoss: 0.587803\n",
            "Train Epoch: 1 [23040/40000 (58%)]\tLoss: 0.308283\n",
            "Train Epoch: 1 [23680/40000 (59%)]\tLoss: 0.372790\n",
            "Train Epoch: 1 [24320/40000 (61%)]\tLoss: 0.334390\n",
            "Train Epoch: 1 [24960/40000 (62%)]\tLoss: 0.390092\n",
            "Train Epoch: 1 [25600/40000 (64%)]\tLoss: 0.394257\n",
            "Train Epoch: 1 [26240/40000 (66%)]\tLoss: 0.338026\n",
            "Train Epoch: 1 [26880/40000 (67%)]\tLoss: 0.427281\n",
            "Train Epoch: 1 [27520/40000 (69%)]\tLoss: 0.421815\n",
            "Train Epoch: 1 [28160/40000 (70%)]\tLoss: 0.452903\n",
            "Train Epoch: 1 [28800/40000 (72%)]\tLoss: 0.354626\n",
            "Train Epoch: 1 [29440/40000 (74%)]\tLoss: 0.351702\n",
            "Train Epoch: 1 [30080/40000 (75%)]\tLoss: 0.350210\n",
            "Train Epoch: 1 [30720/40000 (77%)]\tLoss: 0.478261\n",
            "Train Epoch: 1 [31360/40000 (78%)]\tLoss: 0.368087\n",
            "Train Epoch: 1 [32000/40000 (80%)]\tLoss: 0.502470\n",
            "Train Epoch: 1 [32640/40000 (82%)]\tLoss: 0.371103\n",
            "Train Epoch: 1 [33280/40000 (83%)]\tLoss: 0.376739\n",
            "Train Epoch: 1 [33920/40000 (85%)]\tLoss: 0.414631\n",
            "Train Epoch: 1 [34560/40000 (86%)]\tLoss: 0.292272\n",
            "Train Epoch: 1 [35200/40000 (88%)]\tLoss: 0.345317\n",
            "Train Epoch: 1 [35840/40000 (90%)]\tLoss: 0.364565\n",
            "Train Epoch: 1 [36480/40000 (91%)]\tLoss: 0.323242\n",
            "Train Epoch: 1 [37120/40000 (93%)]\tLoss: 0.346491\n",
            "Train Epoch: 1 [37760/40000 (94%)]\tLoss: 0.533943\n",
            "Train Epoch: 1 [38400/40000 (96%)]\tLoss: 0.401230\n",
            "Train Epoch: 1 [39040/40000 (98%)]\tLoss: 0.237409\n",
            "Train Epoch: 1 [39680/40000 (99%)]\tLoss: 0.253290\n",
            "testing on train set\n",
            "\n",
            "Test set: Average loss: 0.3993, Accuracy: 34024/40000 (85.06%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 1.6773, Accuracy: 2072/20000 (10.36%)\n",
            "\n",
            "Train Epoch: 2 [0/40000 (0%)]\tLoss: 0.374425\n",
            "Train Epoch: 2 [640/40000 (2%)]\tLoss: 0.510446\n",
            "Train Epoch: 2 [1280/40000 (3%)]\tLoss: 0.310221\n",
            "Train Epoch: 2 [1920/40000 (5%)]\tLoss: 0.354607\n",
            "Train Epoch: 2 [2560/40000 (6%)]\tLoss: 0.441992\n",
            "Train Epoch: 2 [3200/40000 (8%)]\tLoss: 0.222560\n",
            "Train Epoch: 2 [3840/40000 (10%)]\tLoss: 0.354080\n",
            "Train Epoch: 2 [4480/40000 (11%)]\tLoss: 0.362705\n",
            "Train Epoch: 2 [5120/40000 (13%)]\tLoss: 0.379182\n",
            "Train Epoch: 2 [5760/40000 (14%)]\tLoss: 0.320859\n",
            "Train Epoch: 2 [6400/40000 (16%)]\tLoss: 0.377313\n",
            "Train Epoch: 2 [7040/40000 (18%)]\tLoss: 0.355247\n",
            "Train Epoch: 2 [7680/40000 (19%)]\tLoss: 0.411477\n",
            "Train Epoch: 2 [8320/40000 (21%)]\tLoss: 0.364409\n",
            "Train Epoch: 2 [8960/40000 (22%)]\tLoss: 0.421817\n",
            "Train Epoch: 2 [9600/40000 (24%)]\tLoss: 0.475550\n",
            "Train Epoch: 2 [10240/40000 (26%)]\tLoss: 0.404923\n",
            "Train Epoch: 2 [10880/40000 (27%)]\tLoss: 0.433137\n",
            "Train Epoch: 2 [11520/40000 (29%)]\tLoss: 0.362994\n",
            "Train Epoch: 2 [12160/40000 (30%)]\tLoss: 0.398600\n",
            "Train Epoch: 2 [12800/40000 (32%)]\tLoss: 0.387588\n",
            "Train Epoch: 2 [13440/40000 (34%)]\tLoss: 0.468939\n",
            "Train Epoch: 2 [14080/40000 (35%)]\tLoss: 0.440189\n",
            "Train Epoch: 2 [14720/40000 (37%)]\tLoss: 0.290316\n",
            "Train Epoch: 2 [15360/40000 (38%)]\tLoss: 0.457030\n",
            "Train Epoch: 2 [16000/40000 (40%)]\tLoss: 0.435064\n",
            "Train Epoch: 2 [16640/40000 (42%)]\tLoss: 0.418902\n",
            "Train Epoch: 2 [17280/40000 (43%)]\tLoss: 0.391064\n",
            "Train Epoch: 2 [17920/40000 (45%)]\tLoss: 0.433691\n",
            "Train Epoch: 2 [18560/40000 (46%)]\tLoss: 0.493995\n",
            "Train Epoch: 2 [19200/40000 (48%)]\tLoss: 0.402955\n",
            "Train Epoch: 2 [19840/40000 (50%)]\tLoss: 0.398036\n",
            "Train Epoch: 2 [20480/40000 (51%)]\tLoss: 0.404256\n",
            "Train Epoch: 2 [21120/40000 (53%)]\tLoss: 0.358731\n",
            "Train Epoch: 2 [21760/40000 (54%)]\tLoss: 0.331488\n",
            "Train Epoch: 2 [22400/40000 (56%)]\tLoss: 0.319513\n",
            "Train Epoch: 2 [23040/40000 (58%)]\tLoss: 0.435365\n",
            "Train Epoch: 2 [23680/40000 (59%)]\tLoss: 0.539722\n",
            "Train Epoch: 2 [24320/40000 (61%)]\tLoss: 0.377145\n",
            "Train Epoch: 2 [24960/40000 (62%)]\tLoss: 0.493029\n",
            "Train Epoch: 2 [25600/40000 (64%)]\tLoss: 0.459871\n",
            "Train Epoch: 2 [26240/40000 (66%)]\tLoss: 0.410087\n",
            "Train Epoch: 2 [26880/40000 (67%)]\tLoss: 0.427006\n",
            "Train Epoch: 2 [27520/40000 (69%)]\tLoss: 0.404207\n",
            "Train Epoch: 2 [28160/40000 (70%)]\tLoss: 0.391667\n",
            "Train Epoch: 2 [28800/40000 (72%)]\tLoss: 0.430792\n",
            "Train Epoch: 2 [29440/40000 (74%)]\tLoss: 0.344182\n",
            "Train Epoch: 2 [30080/40000 (75%)]\tLoss: 0.415688\n",
            "Train Epoch: 2 [30720/40000 (77%)]\tLoss: 0.485024\n",
            "Train Epoch: 2 [31360/40000 (78%)]\tLoss: 0.460278\n",
            "Train Epoch: 2 [32000/40000 (80%)]\tLoss: 0.301649\n",
            "Train Epoch: 2 [32640/40000 (82%)]\tLoss: 0.343718\n",
            "Train Epoch: 2 [33280/40000 (83%)]\tLoss: 0.368603\n",
            "Train Epoch: 2 [33920/40000 (85%)]\tLoss: 0.459892\n",
            "Train Epoch: 2 [34560/40000 (86%)]\tLoss: 0.363754\n",
            "Train Epoch: 2 [35200/40000 (88%)]\tLoss: 0.367901\n",
            "Train Epoch: 2 [35840/40000 (90%)]\tLoss: 0.358679\n",
            "Train Epoch: 2 [36480/40000 (91%)]\tLoss: 0.413931\n",
            "Train Epoch: 2 [37120/40000 (93%)]\tLoss: 0.457369\n",
            "Train Epoch: 2 [37760/40000 (94%)]\tLoss: 0.347556\n",
            "Train Epoch: 2 [38400/40000 (96%)]\tLoss: 0.512546\n",
            "Train Epoch: 2 [39040/40000 (98%)]\tLoss: 0.382591\n",
            "Train Epoch: 2 [39680/40000 (99%)]\tLoss: 0.576352\n",
            "testing on train set\n",
            "\n",
            "Test set: Average loss: 0.4052, Accuracy: 33965/40000 (84.91%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 1.4594, Accuracy: 2296/20000 (11.48%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6wptoCqwjZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1-MgdGuwlCY",
        "colab_type": "text"
      },
      "source": [
        "## IRM\n",
        "\n",
        "After trying lots of hyperparameters and various tricks, I was able to \"sort of\"\n",
        "achieve the results in the paper. I say \"sort of\" because the training process\n",
        "is very unstable and dependent on the random seed. I would say about ~70% of \n",
        "runs will converge to a respectable value (train accuracy > 70%, test accuracy > 60%)\n",
        "after a few tens of epochs.\n",
        "\n",
        "The most common failure case is when the gradient norm penalty term is weighted\n",
        "too highly relative to the ERM term. In this case, Φ converges to a function that \n",
        "returns the same value for all inputs. The classifier cannot recover from this point\n",
        "and naturally, the accuracy is stuck at 50% for all environments.\n",
        "\n",
        "Another failure case is when the gradient norm penalty is too low and the\n",
        "optimization essentially acts as in ERM (train accuracy > 80%, test accuracy ~10%).\n",
        "\n",
        "The most important trick I used to get this to work is through scheduled \n",
        "increase of the gradient norm penalty weight.\n",
        "We start at 0 for the gradient norm penalty weight, essentially beginning as ERM,\n",
        "then slowly increase it per epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kio2CQOdwqA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_irm_penalty(losses, dummy):\n",
        "  g1 = grad(losses[0::2].mean(), dummy, create_graph=True)[0]\n",
        "  g2 = grad(losses[1::2].mean(), dummy, create_graph=True)[0]\n",
        "  return (g1 * g2).sum()\n",
        "\n",
        "\n",
        "def irm_train(model, device, train_loaders, optimizer, epoch):\n",
        "  model.train()\n",
        "\n",
        "  train_loaders = [iter(x) for x in train_loaders]\n",
        "\n",
        "  dummy_w = torch.nn.Parameter(torch.Tensor([1.0])).to(device)\n",
        "\n",
        "  batch_idx = 0\n",
        "  penalty_multiplier = min(epoch//2 * 10., 300.)\n",
        "  print(f'Using penalty multiplier {penalty_multiplier}')\n",
        "  while True:\n",
        "    optimizer.zero_grad()\n",
        "    error = 0\n",
        "    penalty = 0\n",
        "    for loader in train_loaders:\n",
        "      data, target = next(loader, (None, None))\n",
        "      if data is None:\n",
        "        return\n",
        "      data, target = data.to(device), target.to(device).float()\n",
        "      output = model(data)\n",
        "      loss_erm = F.binary_cross_entropy_with_logits(output * dummy_w, target, reduction='none')\n",
        "      penalty += compute_irm_penalty(loss_erm, dummy_w)\n",
        "      error += loss_erm.mean()\n",
        "    (error + penalty_multiplier * penalty).backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 2 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tERM loss: {:.6f}\\tGrad penalty: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loaders[0].dataset),\n",
        "               100. * batch_idx / len(train_loaders[0]), error.item(), penalty.item()))\n",
        "      print('First 20 logits', output.data.cpu().numpy()[:20])\n",
        "\n",
        "    batch_idx += 1\n",
        "\n",
        "\n",
        "def train_and_test_irm():\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "  kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "  train1_loader = torch.utils.data.DataLoader(\n",
        "    ColoredMNIST(root='./data', env='train1',\n",
        "                 transform=transforms.Compose([\n",
        "                     transforms.ToTensor(),\n",
        "                     transforms.Normalize((0.1307, 0.1307, 0.), (0.3081, 0.3081, 0.3081))\n",
        "                   ])),\n",
        "    batch_size=2000, shuffle=True, **kwargs)\n",
        "\n",
        "  train2_loader = torch.utils.data.DataLoader(\n",
        "    ColoredMNIST(root='./data', env='train2',\n",
        "                 transform=transforms.Compose([\n",
        "                     transforms.ToTensor(),\n",
        "                     transforms.Normalize((0.1307, 0.1307, 0.), (0.3081, 0.3081, 0.3081))\n",
        "                   ])),\n",
        "    batch_size=2000, shuffle=True, **kwargs)\n",
        "\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "    ColoredMNIST(root='./data', env='test', transform=transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.1307, 0.1307, 0.), (0.3081, 0.3081, 0.3081))\n",
        "    ])),\n",
        "    batch_size=1000, shuffle=True, **kwargs)\n",
        "\n",
        "  model = ConvNet().to(device)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "  for epoch in range(1, 100):\n",
        "    irm_train(model, device, [train1_loader, train2_loader], optimizer, epoch)\n",
        "    print('testing on train1 set')\n",
        "    test_model(model, device, train1_loader)\n",
        "    print('testing on train2 set')\n",
        "    test_model(model, device, train2_loader)\n",
        "    print('testing on test set')\n",
        "    test_model(model, device, test_loader)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXFWJLc-xm-F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "654b6304-c9c5-451a-bfdf-4dcc93f6f0aa"
      },
      "source": [
        "train_and_test_irm()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Colored MNIST dataset already exists\n",
            "Colored MNIST dataset already exists\n",
            "Colored MNIST dataset already exists\n",
            "Using penalty multiplier 0.0\n",
            "Train Epoch: 1 [0/20000 (0%)]\tERM loss: 1.383896\tGrad penalty: 0.000001\n",
            "First 20 logits [ 0.03538679  0.00063856  0.00051292  0.06880804  0.02827823  0.03978533\n",
            "  0.07892961  0.03324739  0.06085899  0.03023075  0.06109099  0.0790024\n",
            "  0.02939987  0.02613666  0.05465072 -0.01478364  0.04402092  0.04603443\n",
            "  0.06186434  0.04901162]\n",
            "Train Epoch: 1 [4000/20000 (20%)]\tERM loss: 0.982504\tGrad penalty: 0.028032\n",
            "First 20 logits [ 0.5292423  -1.163523    0.79521847  0.500141    1.0006373  -0.95062673\n",
            "  0.5812396  -0.7832521   0.4062093  -0.9933591  -0.94026417  0.5736829\n",
            "  0.40755484 -1.1595995   0.7633655   0.4332226   0.65533626  0.6113759\n",
            "  0.74044985  0.7891848 ]\n",
            "Train Epoch: 1 [8000/20000 (40%)]\tERM loss: 0.861611\tGrad penalty: 0.076652\n",
            "First 20 logits [-1.5507442 -2.3173645 -2.4626696  2.5986905 -2.5328832 -2.0177305\n",
            "  2.3570015 -2.8193827  3.1003923  2.3490822 -2.775993   1.7333978\n",
            " -2.9534483  2.8997064  2.6018305  2.3030353 -2.8175375 -1.5719161\n",
            " -1.8843411  2.9036665]\n",
            "Train Epoch: 1 [12000/20000 (60%)]\tERM loss: 0.939667\tGrad penalty: 0.045020\n",
            "First 20 logits [-1.8277283 -1.3817806 -1.7981374  2.053577   3.4134524  2.883947\n",
            "  3.0831883 -1.589723   3.0405426 -1.679847  -1.7695783  2.8256288\n",
            "  3.2296135  2.6156163  2.727474   2.834915  -1.1954998 -1.5823027\n",
            "  2.6806738 -1.708171 ]\n",
            "Train Epoch: 1 [16000/20000 (80%)]\tERM loss: 0.877536\tGrad penalty: 0.022745\n",
            "First 20 logits [ 1.4135214  1.4163632 -2.5197291  1.5853258  1.5998167  1.40627\n",
            "  1.7678629  1.5396428  1.5926498  1.6475879  1.457075  -2.7541537\n",
            " -2.3022346  1.5668504  1.2978112 -1.9826912  1.4209652 -1.761101\n",
            " -2.3911734 -1.9034855]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.4961, Accuracy: 15995/20000 (79.97%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.3688, Accuracy: 18030/20000 (90.15%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 1.3638, Accuracy: 2072/20000 (10.36%)\n",
            "\n",
            "Using penalty multiplier 10.0\n",
            "Train Epoch: 2 [0/20000 (0%)]\tERM loss: 0.856484\tGrad penalty: 0.023264\n",
            "First 20 logits [-1.835143  -1.2874179 -1.7672346  1.229008  -1.6107953 -1.213826\n",
            " -1.7928954 -1.522737   1.1159604  0.8438001 -1.620855   1.2656673\n",
            " -1.0562105 -1.6575652  1.3069146  1.4121591  1.409555  -1.2032574\n",
            "  1.4155761 -1.460739 ]\n",
            "Train Epoch: 2 [4000/20000 (20%)]\tERM loss: 0.882763\tGrad penalty: 0.015307\n",
            "First 20 logits [-2.0375483 -2.6357572  1.2460946 -2.5717921 -1.8150996  1.2677486\n",
            "  1.1561636  1.3835219  1.1398678 -2.5366216 -2.4770718  1.4179109\n",
            " -1.6593947 -2.4725432  1.1938245  1.4610219  0.8573669 -1.8457649\n",
            " -1.8605176 -2.1938615]\n",
            "Train Epoch: 2 [8000/20000 (40%)]\tERM loss: 0.849875\tGrad penalty: 0.013457\n",
            "First 20 logits [ 1.1672896  -1.7078902   1.2614369   1.785574    1.5794433   1.1089559\n",
            "  1.3357786  -1.6773876  -1.8284823  -1.337447    1.262828    1.1562676\n",
            "  0.75515294  1.0766865  -1.191011   -1.8631854  -0.96001726  1.4360719\n",
            " -1.4221172   1.2870415 ]\n",
            "Train Epoch: 2 [12000/20000 (60%)]\tERM loss: 0.873780\tGrad penalty: 0.031711\n",
            "First 20 logits [ 1.9655266  1.580896  -2.0479982 -2.2296772  1.942912   1.5617863\n",
            "  1.664701   1.7819766  1.9141214 -2.017867  -1.8118223  1.6382049\n",
            "  2.0409524 -1.9551435  1.8603381 -1.75909    1.5857733 -1.8515772\n",
            "  2.028009  -2.512861 ]\n",
            "Train Epoch: 2 [16000/20000 (80%)]\tERM loss: 0.838282\tGrad penalty: 0.015933\n",
            "First 20 logits [ 1.8459218  1.6601833  1.6251754 -1.213198   1.7226152  1.6442931\n",
            " -1.4426568  1.6273186  2.2216358  1.4295578 -1.3976841  2.2240825\n",
            " -1.3683766 -1.3727901 -1.1362542 -1.5891716 -1.1702377 -1.3334982\n",
            "  1.8093388 -1.357182 ]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.5039, Accuracy: 15995/20000 (79.97%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.3534, Accuracy: 18030/20000 (90.15%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 1.5206, Accuracy: 2072/20000 (10.36%)\n",
            "\n",
            "Using penalty multiplier 10.0\n",
            "Train Epoch: 3 [0/20000 (0%)]\tERM loss: 0.859977\tGrad penalty: 0.009669\n",
            "First 20 logits [-0.8396057  1.95045   -0.6984908 -1.707097   1.9760107  1.8800093\n",
            "  2.2125022 -1.5140082  2.08036    1.9286674 -1.0266541 -1.0252935\n",
            "  1.9117876  2.0059063 -1.3132789  2.04302   -0.6948777  2.3567367\n",
            " -0.8871441  2.2038205]\n",
            "Train Epoch: 3 [4000/20000 (20%)]\tERM loss: 0.833765\tGrad penalty: 0.018232\n",
            "First 20 logits [-1.4296255  1.988652   2.3560503 -1.0931121  2.016609   1.536292\n",
            "  2.2192478  2.4169488  2.8183367  2.768414  -0.7455881  2.1724367\n",
            " -1.3719637  1.9082714 -1.3742671 -1.6299889  2.6697767  2.546413\n",
            "  2.3564425 -1.8092796]\n",
            "Train Epoch: 3 [8000/20000 (40%)]\tERM loss: 0.810502\tGrad penalty: 0.009284\n",
            "First 20 logits [-1.6113138   1.9599949   2.415232   -1.5440707   2.1155293  -0.859487\n",
            " -0.88916355  2.4008207   1.9242245  -1.4122357   2.5261767  -1.372561\n",
            "  2.2919035  -1.6324955   1.5998331  -1.288399   -1.1003408   1.9622669\n",
            " -1.092825    1.8632156 ]\n",
            "Train Epoch: 3 [12000/20000 (60%)]\tERM loss: 0.832716\tGrad penalty: 0.016295\n",
            "First 20 logits [-1.6816481  1.6791548  1.9257098  1.5100694 -1.6844267  1.6060137\n",
            "  1.6393198 -1.7886682 -1.1869818  1.480905   0.7201524  1.9630178\n",
            "  1.5153112  1.6526064 -1.6514019 -1.394969   1.7798961 -1.5598451\n",
            "  1.6985033  1.7970439]\n",
            "Train Epoch: 3 [16000/20000 (80%)]\tERM loss: 0.824328\tGrad penalty: 0.010094\n",
            "First 20 logits [-2.546003   -1.9713801  -2.1068602  -2.403662   -1.7340105   1.7522937\n",
            " -2.2637105   1.5756468  -2.1903653  -2.2136548   1.6275564  -1.4564086\n",
            "  1.1992832  -2.1658342   1.552928    0.95471424 -2.6012669  -2.2190614\n",
            " -2.339992    0.6571161 ]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.4916, Accuracy: 15995/20000 (79.97%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.3304, Accuracy: 18030/20000 (90.15%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 1.6072, Accuracy: 2072/20000 (10.36%)\n",
            "\n",
            "Using penalty multiplier 20.0\n",
            "Train Epoch: 4 [0/20000 (0%)]\tERM loss: 0.829078\tGrad penalty: 0.014157\n",
            "First 20 logits [ 1.1249006   1.5920814   2.1640766   0.62289226 -2.9033985   1.300177\n",
            " -1.4350578  -1.4682385   0.80240107 -2.0120301   1.6718501   1.1469194\n",
            "  1.5284365   1.6403407  -2.327003   -1.9557453  -1.3697422   2.1833339\n",
            "  1.5390639  -1.9628404 ]\n",
            "Train Epoch: 4 [4000/20000 (20%)]\tERM loss: 0.810948\tGrad penalty: 0.009951\n",
            "First 20 logits [-2.380179   1.0027996 -1.591975  -2.0331216 -1.2820686 -2.385717\n",
            "  1.8067615 -1.9571921  1.0131009  1.2767034  1.3927971 -1.9059347\n",
            " -2.10198    1.9607717 -1.7284719  1.3840599 -1.3727719 -1.7408352\n",
            " -2.2079268 -1.192057 ]\n",
            "Train Epoch: 4 [8000/20000 (40%)]\tERM loss: 0.810856\tGrad penalty: 0.012350\n",
            "First 20 logits [ 1.5757473   1.8221047   1.590519    0.8618017  -1.574997   -1.2694424\n",
            "  0.49403733  1.3737514  -2.6117616   1.3341435  -2.8230038   1.3205976\n",
            "  0.64676094 -2.3625548   1.394134   -1.2058612   1.2786669  -1.8378965\n",
            "  0.9749171   1.9892204 ]\n",
            "Train Epoch: 4 [12000/20000 (60%)]\tERM loss: 0.808235\tGrad penalty: 0.015044\n",
            "First 20 logits [ 2.164944    1.2759528   1.8335212   1.5364542   2.0175853   1.3185171\n",
            " -0.79159397  2.2082605  -1.2655104   1.3189584   1.7914954  -2.036572\n",
            "  1.0128658  -1.5010848  -1.8187035  -1.9379297  -1.5990595   1.7050447\n",
            "  1.6772786   1.9741559 ]\n",
            "Train Epoch: 4 [16000/20000 (80%)]\tERM loss: 0.792354\tGrad penalty: 0.020248\n",
            "First 20 logits [-1.6930026  2.3418446 -2.8070421 -2.9615452  1.3880224  2.297496\n",
            " -3.113778  -2.8075933 -1.4797033 -1.0936985  2.579127  -1.8670009\n",
            " -0.8416194 -1.1489456 -2.8343813 -2.293899   2.5231988 -2.6648712\n",
            " -1.3638299  2.6366255]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.4721, Accuracy: 15996/20000 (79.98%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.3413, Accuracy: 18014/20000 (90.07%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 1.3881, Accuracy: 2137/20000 (10.69%)\n",
            "\n",
            "Using penalty multiplier 20.0\n",
            "Train Epoch: 5 [0/20000 (0%)]\tERM loss: 0.817812\tGrad penalty: 0.013205\n",
            "First 20 logits [ 1.9272302   1.7128475   2.5398347  -2.063647    1.1121078   1.986542\n",
            " -1.041155    1.7736051  -0.31303316  1.843077   -1.2001628  -2.2239366\n",
            " -1.9303633  -2.4794247   1.4310154   2.3390698   1.6408768   1.801556\n",
            " -1.5058419  -1.1475567 ]\n",
            "Train Epoch: 5 [4000/20000 (20%)]\tERM loss: 0.868903\tGrad penalty: 0.007736\n",
            "First 20 logits [-0.48166126  0.11175244  2.0745254  -0.39083898  2.3093598   2.3483527\n",
            "  0.7456025   2.3768485   3.0420086   1.801864   -1.8305796   1.6109018\n",
            "  2.6919854   1.5036387   2.7711365   1.4523325   1.9395059  -0.9279119\n",
            " -0.9960732   2.1855655 ]\n",
            "Train Epoch: 5 [8000/20000 (40%)]\tERM loss: 0.901205\tGrad penalty: 0.010257\n",
            "First 20 logits [-0.07935297 -0.49905846 -0.003448   -1.6557341  -1.6528313  -1.5776439\n",
            "  1.1713886  -2.2467923   0.21302363 -0.4448537   3.0399745  -2.1556342\n",
            "  2.1754758   2.707164    1.0400952  -0.6540968  -1.5294858  -0.10483874\n",
            " -1.403986    1.9153106 ]\n",
            "Train Epoch: 5 [12000/20000 (60%)]\tERM loss: 0.813053\tGrad penalty: 0.004996\n",
            "First 20 logits [-0.9638989   2.4128892   1.11179     2.06665     1.7489171   1.6939708\n",
            "  1.2110933   0.6889876   1.6555246   1.2554384   2.574973    2.6859422\n",
            "  2.359337    2.3225214  -2.7843354   1.2092558   2.5943563   1.4840908\n",
            " -2.4137197  -0.73990965]\n",
            "Train Epoch: 5 [16000/20000 (80%)]\tERM loss: 0.806759\tGrad penalty: 0.008662\n",
            "First 20 logits [-3.454794    1.0396053   1.9341813   1.6327779  -1.566628    0.9485689\n",
            " -1.310128    1.4726102  -1.2462913  -1.1486579  -3.9199042  -3.8336475\n",
            " -0.7791224  -3.479292    1.2704716   1.1187538   1.1371336   1.9555258\n",
            " -0.86989444  1.2646772 ]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.4792, Accuracy: 15887/20000 (79.44%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.3492, Accuracy: 17650/20000 (88.25%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 1.4357, Accuracy: 3627/20000 (18.14%)\n",
            "\n",
            "Using penalty multiplier 30.0\n",
            "Train Epoch: 6 [0/20000 (0%)]\tERM loss: 0.810673\tGrad penalty: 0.006197\n",
            "First 20 logits [-3.8471649   0.6533224   0.01088318 -0.7465192  -0.989727   -2.395497\n",
            " -4.010339   -1.4380845   1.1548238  -2.423466   -3.157326   -2.8747702\n",
            "  1.1747482   1.5685071   1.6926956   0.29081708  1.6481866  -3.697967\n",
            "  0.6337789  -3.9224975 ]\n",
            "Train Epoch: 6 [4000/20000 (20%)]\tERM loss: 0.885993\tGrad penalty: 0.004927\n",
            "First 20 logits [-0.1730503  -0.7726476  -3.651311   -3.274838   -3.1451664   1.8726493\n",
            "  0.7903445   0.7831575  -3.898044   -0.77046853  1.61048     0.6931493\n",
            " -1.7000054   1.4522661  -0.6169705   1.2613094  -2.8609562  -3.1080384\n",
            "  0.5388243  -0.7652508 ]\n",
            "Train Epoch: 6 [8000/20000 (40%)]\tERM loss: 0.903565\tGrad penalty: 0.004106\n",
            "First 20 logits [-1.4632313  -0.72915334 -2.103725    1.4164156  -0.27730426 -3.1880302\n",
            "  2.0651536  -2.4816961   1.2677432   0.40435252 -0.2023599   0.05822557\n",
            "  1.2446934  -1.0703162  -2.6749063  -1.9136599  -3.0793967   1.3861095\n",
            " -2.101241   -0.79838043]\n",
            "Train Epoch: 6 [12000/20000 (60%)]\tERM loss: 0.928876\tGrad penalty: 0.003630\n",
            "First 20 logits [-2.522449    1.1990093  -0.9617578  -3.2292078  -3.096647    0.03116857\n",
            " -2.5697768  -0.15355748  1.5685183  -0.38616708  0.7243843   0.43931767\n",
            " -0.13947031 -0.0145741   0.44112608 -0.2902498   1.1486195  -0.7182318\n",
            " -1.0020652  -0.9359562 ]\n",
            "Train Epoch: 6 [16000/20000 (80%)]\tERM loss: 0.937365\tGrad penalty: 0.002352\n",
            "First 20 logits [ 1.6173531  -2.5273232  -1.9371989   0.37629527 -1.3590101   0.9645862\n",
            "  0.11958492 -0.1424834  -3.6752427  -1.7813407   0.5163078   0.12354909\n",
            "  1.9760063   0.01048823 -0.24773055 -0.99620295 -1.525355   -2.5392234\n",
            "  2.2052414  -3.6354718 ]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.5017, Accuracy: 15053/20000 (75.27%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.4234, Accuracy: 15880/20000 (79.40%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 1.1182, Accuracy: 8398/20000 (41.99%)\n",
            "\n",
            "Using penalty multiplier 30.0\n",
            "Train Epoch: 7 [0/20000 (0%)]\tERM loss: 0.950917\tGrad penalty: 0.004070\n",
            "First 20 logits [ 0.16998693  1.9345994   1.1608173  -0.3921907  -0.98348504 -0.7351643\n",
            " -3.0870264   0.13612266  1.6922568   1.0167998  -2.4052312  -2.605656\n",
            " -3.038453    2.1398332   1.4539431  -0.30912393  0.86416405 -1.7943318\n",
            " -2.5805418   1.4146173 ]\n",
            "Train Epoch: 7 [4000/20000 (20%)]\tERM loss: 0.844940\tGrad penalty: 0.006137\n",
            "First 20 logits [ 0.70341295  0.28784704  0.46104607 -3.2215147   1.8959582   0.5008912\n",
            " -3.732009    1.6632134   2.597731   -3.3794734  -0.07319163 -3.111662\n",
            " -2.3995662   1.8746332  -2.0174258   2.17473     0.5250518  -2.277357\n",
            "  0.2250919   2.5061502 ]\n",
            "Train Epoch: 7 [8000/20000 (40%)]\tERM loss: 0.813749\tGrad penalty: 0.002136\n",
            "First 20 logits [-2.0430644   0.21807577  2.6473439  -1.0495211   3.2861247   0.79162943\n",
            "  1.726428    0.4652052  -3.0408652  -3.1665761  -3.5407073  -3.3879597\n",
            " -3.5281408  -2.8499851   2.939781    1.4087858  -1.4492123   1.3031328\n",
            "  2.0697303   2.290609  ]\n",
            "Train Epoch: 7 [12000/20000 (60%)]\tERM loss: 0.824145\tGrad penalty: 0.006177\n",
            "First 20 logits [ 1.3813548  -2.0942516   2.4713535  -3.1809027   1.1518776   0.02975891\n",
            "  0.28266746  2.8535335  -2.5500855   0.05671209 -1.5387748   2.2991843\n",
            "  1.9677933   0.0194404  -2.0175512  -0.85865355 -1.8534164   2.7669356\n",
            " -2.8844223  -3.4967978 ]\n",
            "Train Epoch: 7 [16000/20000 (80%)]\tERM loss: 0.831684\tGrad penalty: 0.008143\n",
            "First 20 logits [-1.6528301  -0.0930564  -0.79249823  1.2517259  -2.6566823  -1.5346978\n",
            " -3.4064553  -2.8180513   2.5436275   2.4130375   0.6115976   0.21981312\n",
            " -0.46460274  0.21014734 -1.5993435   2.2357574  -1.0786684   3.1719515\n",
            "  0.02947697 -2.2208073 ]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.4833, Accuracy: 15387/20000 (76.94%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.3920, Accuracy: 16345/20000 (81.72%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 1.1873, Accuracy: 7676/20000 (38.38%)\n",
            "\n",
            "Using penalty multiplier 40.0\n",
            "Train Epoch: 8 [0/20000 (0%)]\tERM loss: 0.871003\tGrad penalty: 0.003295\n",
            "First 20 logits [-1.2336692   1.219553   -0.08330016  1.8587945   2.3243535   0.80250484\n",
            " -2.4693267   1.8925008   2.840776    2.854398   -0.5207028   1.1592717\n",
            " -1.567078    3.0020833  -1.9737704   1.5116502   2.0794291   1.5725727\n",
            "  0.7794342   0.5612873 ]\n",
            "Train Epoch: 8 [4000/20000 (20%)]\tERM loss: 0.877673\tGrad penalty: 0.002213\n",
            "First 20 logits [-2.551278   -3.1375978  -2.0237474   1.2843987   0.08322004 -2.0421085\n",
            "  0.6966766  -0.7188244   0.5825686  -2.8893375  -0.5620507   0.26918173\n",
            " -0.5735857   3.402945    2.484534    3.3235216  -1.0693327   1.1954173\n",
            " -3.7262104   2.7301607 ]\n",
            "Train Epoch: 8 [8000/20000 (40%)]\tERM loss: 0.874429\tGrad penalty: 0.002764\n",
            "First 20 logits [ 0.17402613  2.2556944  -0.46976283 -0.2589371   1.6074522   1.148477\n",
            " -3.3243108   1.1047153   1.7675439  -4.189274    1.794256    1.1446483\n",
            " -3.1978738  -1.069712    0.2363672  -1.3670208   0.49917305  2.472343\n",
            " -2.5123358  -3.232236  ]\n",
            "Train Epoch: 8 [12000/20000 (60%)]\tERM loss: 0.908480\tGrad penalty: 0.001224\n",
            "First 20 logits [-1.248581   -2.5815804  -0.1592278   1.6351358  -3.2898202   1.0624219\n",
            " -0.9109617   0.64253294  0.6597906   2.4338062  -2.8997667   0.3119123\n",
            " -1.5124925   1.287767    0.13631785  0.33821023 -0.4098499   0.10609244\n",
            "  1.0587589  -1.2430297 ]\n",
            "Train Epoch: 8 [16000/20000 (80%)]\tERM loss: 0.930232\tGrad penalty: 0.001058\n",
            "First 20 logits [-0.29274955 -1.7268308   1.0168018  -0.43661788 -3.0088992   2.1255345\n",
            " -1.6957481   0.73501325 -0.27729258 -2.5549016  -0.6815673   1.602344\n",
            " -3.6557543  -2.8017638  -2.3372114   0.95040995 -0.24928018 -1.349183\n",
            " -1.0562317  -0.7120321 ]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.4766, Accuracy: 15461/20000 (77.31%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.4035, Accuracy: 16193/20000 (80.97%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 1.0714, Accuracy: 8548/20000 (42.74%)\n",
            "\n",
            "Using penalty multiplier 40.0\n",
            "Train Epoch: 9 [0/20000 (0%)]\tERM loss: 0.866249\tGrad penalty: 0.003679\n",
            "First 20 logits [ 0.13118082  0.36503017  2.7418487  -0.10528512 -2.647586   -0.91520894\n",
            " -1.310286   -2.5032415  -2.8903131   2.8075297   1.1298972   2.1215386\n",
            "  1.557007   -1.7678938   0.11022297  2.1692038   1.644605   -0.8781644\n",
            " -3.5516636  -2.5472443 ]\n",
            "Train Epoch: 9 [4000/20000 (20%)]\tERM loss: 0.896523\tGrad penalty: 0.007579\n",
            "First 20 logits [ 2.9079533  -2.851886    0.8287846   2.929195   -1.3004004   0.92043453\n",
            "  0.46001905 -0.86144745  3.1152618  -3.091552   -0.4951465  -0.46100867\n",
            "  2.5322096   0.7352939   2.2983816  -2.5172596  -1.742686    3.4379835\n",
            " -1.6997457   3.393696  ]\n",
            "Train Epoch: 9 [8000/20000 (40%)]\tERM loss: 0.907267\tGrad penalty: 0.001753\n",
            "First 20 logits [ 2.5817945   2.363419   -2.3357525  -0.13501969 -1.5477232  -1.5563256\n",
            " -2.7612307   0.9998125  -0.7726165  -0.6808585  -2.6949046   2.547207\n",
            "  2.2892551   1.0150504  -2.2344544  -2.7289045   3.474587    2.5512505\n",
            " -1.0475585   2.0144475 ]\n",
            "Train Epoch: 9 [12000/20000 (60%)]\tERM loss: 0.882562\tGrad penalty: 0.004430\n",
            "First 20 logits [-1.9525439  -3.0678387  -1.4677143  -2.515179    2.0500193   2.4759617\n",
            "  1.7625731  -0.9873148   0.8112786   1.0827318  -1.6852281  -0.8937254\n",
            " -1.4848672  -2.1468103   0.7549617  -1.2216332   0.5035541  -2.7943883\n",
            "  1.5284657  -0.21888418]\n",
            "Train Epoch: 9 [16000/20000 (80%)]\tERM loss: 0.896295\tGrad penalty: 0.002034\n",
            "First 20 logits [-2.1328917   0.87313807 -3.625479    1.2515776   0.14912032 -2.1076856\n",
            "  2.2185717  -3.3566015  -3.3425207  -2.6236463   1.6467112  -1.9922004\n",
            "  1.4627087  -2.8744457   2.8671541   2.3779683  -1.639659    0.3939552\n",
            "  1.1448563  -3.0522888 ]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.4767, Accuracy: 15396/20000 (76.98%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.4008, Accuracy: 16080/20000 (80.40%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 1.1089, Accuracy: 9038/20000 (45.19%)\n",
            "\n",
            "Using penalty multiplier 50.0\n",
            "Train Epoch: 10 [0/20000 (0%)]\tERM loss: 0.865272\tGrad penalty: 0.002250\n",
            "First 20 logits [-2.5744078 -2.735831   3.2206335 -1.6862192  1.6544229 -1.4078888\n",
            " -2.3856282  2.4495149  2.5030107  2.5356174  2.385284   2.4853375\n",
            "  2.3045065  2.9306865  2.1767378 -3.210569  -0.018475   2.1850207\n",
            "  2.924697   0.860579 ]\n",
            "Train Epoch: 10 [4000/20000 (20%)]\tERM loss: 0.843763\tGrad penalty: 0.000588\n",
            "First 20 logits [-0.50821817 -1.5536958   1.0564377  -0.9229372   2.618714    0.77063817\n",
            " -0.07872614  0.853743   -1.4255933  -0.6411058  -1.2903746  -1.3274481\n",
            " -1.0958169   1.0624647  -0.7997461   0.3927481  -1.416596    2.533361\n",
            "  2.3564215  -3.9921362 ]\n",
            "Train Epoch: 10 [8000/20000 (40%)]\tERM loss: 0.843443\tGrad penalty: 0.001997\n",
            "First 20 logits [-2.5546026   1.8035337   2.7521672  -2.701879   -1.6027255  -0.8117497\n",
            " -0.66038007  0.38023242 -3.3719807   2.5329971   0.0728369   2.0592248\n",
            "  1.3533118   1.7351593  -0.73359936  2.436144    1.886231   -2.023606\n",
            "  2.0347703  -1.5786312 ]\n",
            "Train Epoch: 10 [12000/20000 (60%)]\tERM loss: 0.874383\tGrad penalty: 0.001500\n",
            "First 20 logits [ 1.870111    2.413398    1.2743456  -2.2124877  -0.47243845  2.0781555\n",
            " -3.0464306  -2.8001554  -0.50238895  2.7866476  -4.0993786   0.48888516\n",
            " -1.8403039   2.6907134  -2.5635283  -0.25755566 -1.6524674   1.3679419\n",
            " -0.11231787  2.4561725 ]\n",
            "Train Epoch: 10 [16000/20000 (80%)]\tERM loss: 0.913015\tGrad penalty: 0.001601\n",
            "First 20 logits [-2.0058167  -3.8665779  -2.931813   -2.4950647   0.09719387 -0.50534505\n",
            "  0.46275294 -3.028535    2.652215   -3.0179164  -1.3380275  -0.20331392\n",
            "  3.133721    2.8484657  -0.36205298  2.3975992   0.6762864   2.652015\n",
            "  2.3128653  -1.2308877 ]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.4951, Accuracy: 15039/20000 (75.19%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.4437, Accuracy: 15495/20000 (77.47%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 0.9399, Accuracy: 10480/20000 (52.40%)\n",
            "\n",
            "Using penalty multiplier 50.0\n",
            "Train Epoch: 11 [0/20000 (0%)]\tERM loss: 0.937407\tGrad penalty: 0.001067\n",
            "First 20 logits [-2.1136117   1.1739949  -0.11178295 -0.59665823  2.234857    2.2061548\n",
            " -0.9327486  -1.6439853   2.1441894  -0.25626794 -0.38737047 -3.1685627\n",
            " -1.9080437  -1.5275006  -2.0832999   2.3122883   0.9583164  -2.643332\n",
            "  0.71148443  1.4675531 ]\n",
            "Train Epoch: 11 [4000/20000 (20%)]\tERM loss: 0.928437\tGrad penalty: 0.002615\n",
            "First 20 logits [ 0.5799805  -0.53785217  2.2319443  -0.39400214 -1.9578302   2.5004363\n",
            " -0.4357139  -1.7204287   1.35134     0.7394325  -0.5738307  -2.0592809\n",
            "  1.1270611   0.959658   -1.6240379   0.60955113  0.01917611 -1.7060734\n",
            " -1.5632863   2.0911088 ]\n",
            "Train Epoch: 11 [8000/20000 (40%)]\tERM loss: 0.947062\tGrad penalty: 0.000964\n",
            "First 20 logits [-0.28789684  2.3774529   2.4645152  -2.4228127   2.9075065   3.3955142\n",
            "  0.60351104  1.9166211   1.045208   -1.2318157  -1.616359    0.80449605\n",
            "  0.17857826  1.946614   -0.7077724   1.2892675  -0.8751817  -2.1914997\n",
            " -1.8338305  -0.5882488 ]\n",
            "Train Epoch: 11 [12000/20000 (60%)]\tERM loss: 0.896212\tGrad penalty: 0.001305\n",
            "First 20 logits [ 1.9583176  -2.4119024  -0.85891193  2.3877418  -2.2048779   2.974948\n",
            "  3.1115391   1.6356915   1.2525057  -4.154031    2.6477602   2.8986676\n",
            "  1.0334059   1.7108585   2.3884532   2.5628264  -2.7237484   1.6052029\n",
            "  1.5314202   2.8384962 ]\n",
            "Train Epoch: 11 [16000/20000 (80%)]\tERM loss: 0.833895\tGrad penalty: 0.002122\n",
            "First 20 logits [-1.5614202  -2.0103285  -3.0486383   2.6937497   1.971973    0.19798349\n",
            "  1.412098   -3.6286073   0.5280659  -0.1994204   0.55997854  2.2040234\n",
            "  2.3214169  -2.7228494   1.0882454  -0.77783525 -2.6489835   0.84200966\n",
            " -0.40644467  2.0273197 ]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.4636, Accuracy: 15642/20000 (78.21%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.3693, Accuracy: 16571/20000 (82.86%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 1.2584, Accuracy: 7486/20000 (37.43%)\n",
            "\n",
            "Using penalty multiplier 60.0\n",
            "Train Epoch: 12 [0/20000 (0%)]\tERM loss: 0.837140\tGrad penalty: 0.003161\n",
            "First 20 logits [ 2.5502188   2.302306    2.4779558  -0.87587893  0.03824932  1.9723495\n",
            "  0.24470805 -3.142359    2.2751675   0.3483258   0.39114416  2.6270945\n",
            " -3.9279292   1.8594464  -5.069487    2.469438    2.7156472   1.0082297\n",
            " -0.11764543 -0.46036363]\n",
            "Train Epoch: 12 [4000/20000 (20%)]\tERM loss: 0.871078\tGrad penalty: 0.002898\n",
            "First 20 logits [-3.182759    2.576958   -3.2828753   0.23560068  0.48550355  0.8922772\n",
            "  1.6715393   0.20970151 -0.07485127  1.4724394  -0.6236485  -2.9966216\n",
            " -0.5558722  -2.6877584   0.9297147   2.0848503  -0.17490758  1.5632159\n",
            "  0.18687999  2.0595622 ]\n",
            "Train Epoch: 12 [8000/20000 (40%)]\tERM loss: 0.923027\tGrad penalty: 0.000485\n",
            "First 20 logits [ 1.2696362   1.9803334   0.776501   -3.1173184  -0.53361565  2.9090998\n",
            "  1.9236764  -2.4373934  -2.4379978  -1.8391315  -1.8759842  -0.648857\n",
            "  0.2286766  -1.8290399   1.7066259   1.6658711   0.8693559  -2.3278992\n",
            "  0.56249475  2.1655931 ]\n",
            "Train Epoch: 12 [12000/20000 (60%)]\tERM loss: 0.943237\tGrad penalty: 0.000755\n",
            "First 20 logits [ 0.46779433  2.2044468   0.41885573  2.3252592   1.1867826   1.8428388\n",
            "  3.4641764  -0.7106638   1.7709087   1.2802919   2.178754   -1.3064033\n",
            " -0.389146    2.9322739  -2.0676868   2.3829892  -0.18807736  0.80235004\n",
            "  0.766954    1.6967782 ]\n",
            "Train Epoch: 12 [16000/20000 (80%)]\tERM loss: 0.919040\tGrad penalty: 0.000651\n",
            "First 20 logits [-4.0842009e-01  1.8515544e+00  2.6564362e+00  1.9135567e+00\n",
            " -3.2437673e+00  1.1642760e-01 -5.2068025e-01  9.2204803e-01\n",
            "  2.2015175e-01 -5.4298714e-04  2.1453652e+00 -2.1503496e+00\n",
            " -1.3945765e+00 -1.2111002e-01 -2.7452740e-01  2.5300505e+00\n",
            " -3.1530622e-01 -9.5528930e-01  1.6614498e+00 -2.1150146e+00]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.4711, Accuracy: 15396/20000 (76.98%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.4054, Accuracy: 16083/20000 (80.42%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 1.0567, Accuracy: 9001/20000 (45.01%)\n",
            "\n",
            "Using penalty multiplier 60.0\n",
            "Train Epoch: 13 [0/20000 (0%)]\tERM loss: 0.874435\tGrad penalty: 0.000488\n",
            "First 20 logits [-0.26301706 -0.54315054 -2.747696   -1.1590271   2.8563886  -0.43311173\n",
            "  2.2790265   1.960084   -0.68099415  1.2792268  -3.100264    0.8645714\n",
            " -0.4175964  -0.2035541  -1.5279748  -1.8224543  -1.7860324   1.781113\n",
            " -3.1730883   2.2037451 ]\n",
            "Train Epoch: 13 [4000/20000 (20%)]\tERM loss: 0.891128\tGrad penalty: 0.006221\n",
            "First 20 logits [ 1.1718389  -0.69857913 -2.5315895   1.7045695  -4.027523   -1.0694586\n",
            " -2.28779     0.7548437  -2.5170324  -3.5835106   0.04890122 -1.870221\n",
            "  2.11719    -0.41106305 -3.355861    0.5976128   2.8729904   1.6382469\n",
            " -0.8419728  -1.3263012 ]\n",
            "Train Epoch: 13 [8000/20000 (40%)]\tERM loss: 0.847500\tGrad penalty: 0.003838\n",
            "First 20 logits [ 2.3592937  -2.6092224  -1.7215028  -0.8148246  -0.20379283  1.9561532\n",
            "  1.3344665   2.2616458  -1.3201143  -0.3044942   0.9282405  -0.20606206\n",
            "  0.42008257  1.9516866   1.5860066   1.9784365  -0.35637665  1.8583137\n",
            " -0.35768205  0.45419717]\n",
            "Train Epoch: 13 [12000/20000 (60%)]\tERM loss: 0.864071\tGrad penalty: 0.003096\n",
            "First 20 logits [-2.8055933  -0.6309095   2.9884217  -0.28224793 -0.9258358   3.3404107\n",
            " -1.256847   -0.75492096 -1.8209205  -2.9027596  -3.0424829  -1.7458593\n",
            "  3.7425253   0.42696816 -2.7912843   3.2061732  -3.9087625   0.62172884\n",
            "  2.3439395  -3.5956407 ]\n",
            "Train Epoch: 13 [16000/20000 (80%)]\tERM loss: 0.868903\tGrad penalty: 0.000367\n",
            "First 20 logits [ 0.7478454  -1.7229275  -3.6101308  -1.2380185   0.85197383 -3.304452\n",
            " -0.18551926  3.3014326   1.2250323   0.5764781  -3.125722   -2.317811\n",
            "  2.4648342   2.6898112  -2.0674877   2.9548316  -2.6562505   0.3629582\n",
            "  1.7378194  -0.16650108]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.4958, Accuracy: 15080/20000 (75.40%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.4548, Accuracy: 15350/20000 (76.75%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 0.9121, Accuracy: 10891/20000 (54.45%)\n",
            "\n",
            "Using penalty multiplier 70.0\n",
            "Train Epoch: 14 [0/20000 (0%)]\tERM loss: 0.944806\tGrad penalty: 0.000358\n",
            "First 20 logits [-1.7303549  -2.1085508  -3.9127598  -1.3866982  -2.8724532   0.4336787\n",
            " -0.2145376  -0.18079756 -3.0733774  -0.9055523  -0.18403186  0.2876796\n",
            " -2.6525686   1.2687058   1.0253736   1.393898    1.3442756  -0.4883563\n",
            "  2.3027825  -1.2611231 ]\n",
            "Train Epoch: 14 [4000/20000 (20%)]\tERM loss: 1.012164\tGrad penalty: 0.001497\n",
            "First 20 logits [ 0.13373233 -1.7666229  -1.3712994  -3.078096    0.86036515  0.1224239\n",
            " -3.621741   -0.40266624 -2.1252291  -3.3300552  -0.7425896   0.4191919\n",
            " -0.5399713   0.82321227  2.4144232  -3.7710924   0.7958435  -2.3561678\n",
            "  1.4693526   1.6114386 ]\n",
            "Train Epoch: 14 [8000/20000 (40%)]\tERM loss: 0.914681\tGrad penalty: 0.003009\n",
            "First 20 logits [ 0.10298589  1.5836481  -2.6527689   0.04264266 -2.7198226  -2.817666\n",
            "  0.7404774  -0.89271563  1.1512032  -1.4712198   0.95996934  1.5328512\n",
            " -1.1282808  -2.364626    2.070925    0.54058045  1.5740243  -1.7376915\n",
            " -2.043018   -2.0917976 ]\n",
            "Train Epoch: 14 [12000/20000 (60%)]\tERM loss: 0.911614\tGrad penalty: 0.001748\n",
            "First 20 logits [ 0.04031741  0.47258997  0.1157089   0.16739228  2.2066677  -2.3886719\n",
            " -1.7727209   2.764909    1.3957165  -3.251479   -2.6745398  -1.9147006\n",
            "  0.31867456  2.2990794   0.7194554  -2.837165    1.9392271  -0.18095526\n",
            "  2.2038016  -2.1218739 ]\n",
            "Train Epoch: 14 [16000/20000 (80%)]\tERM loss: 0.894419\tGrad penalty: 0.001886\n",
            "First 20 logits [-0.1018782   0.39118475 -3.225571    0.3734242   2.1962795   0.33691165\n",
            "  1.7184818  -2.654908    0.72940695  2.1856365   2.0893898   2.4909503\n",
            "  1.3710719  -2.144728    2.8429565   3.1088223  -1.5555209   1.4792411\n",
            " -2.4217005   0.30803362]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.4678, Accuracy: 15333/20000 (76.67%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.4042, Accuracy: 15886/20000 (79.43%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 1.0382, Accuracy: 9791/20000 (48.95%)\n",
            "\n",
            "Using penalty multiplier 70.0\n",
            "Train Epoch: 15 [0/20000 (0%)]\tERM loss: 0.868710\tGrad penalty: 0.000193\n",
            "First 20 logits [ 3.2230327   1.7591769  -1.009611    3.972583   -0.05040332 -1.6062049\n",
            "  2.2672675  -0.02100456  2.1334417  -1.679293    1.6172698   0.12411132\n",
            "  0.6966816   0.8826893   2.7835824   0.07135438  2.7215245   1.4619728\n",
            "  1.4018945  -1.9023161 ]\n",
            "Train Epoch: 15 [4000/20000 (20%)]\tERM loss: 0.856146\tGrad penalty: 0.001634\n",
            "First 20 logits [-3.0210423   1.9583367  -3.4720726   0.10739266  2.5230575  -0.19938378\n",
            "  3.2801783  -1.0141474   0.63301027  1.4257386   0.9172299   0.28592575\n",
            " -1.4812278  -0.28363824  2.4875374   1.8942677   0.62156177  2.0084727\n",
            " -1.3614148  -0.77365375]\n",
            "Train Epoch: 15 [8000/20000 (40%)]\tERM loss: 0.835697\tGrad penalty: 0.000898\n",
            "First 20 logits [-3.342442    1.7424245  -3.2091672  -3.3252473   1.7210721  -2.121901\n",
            "  2.4922283   2.4094949   0.7366179   2.883749   -2.8202634  -2.8076212\n",
            " -2.3575766   2.3556147  -1.8831062   0.9863346  -0.4722419   0.25949764\n",
            "  2.5509233  -2.7626944 ]\n",
            "Train Epoch: 15 [12000/20000 (60%)]\tERM loss: 0.857743\tGrad penalty: 0.000447\n",
            "First 20 logits [-3.2594652   1.8839062   2.330882   -4.0368433  -2.2702386   0.8888252\n",
            " -3.2589815   2.3373659  -2.708314   -2.6278648   1.2217668  -2.3274364\n",
            "  1.8915987   2.2681694   2.3284533   0.8245611  -0.19493815 -3.4546437\n",
            "  2.1036313  -3.2400215 ]\n",
            "Train Epoch: 15 [16000/20000 (80%)]\tERM loss: 0.856928\tGrad penalty: 0.002252\n",
            "First 20 logits [-0.6094236  -0.3787222  -1.108156    0.5606527   1.4874572  -0.42350483\n",
            "  0.17877364  0.22054031  2.231859    0.14984232  2.061329   -1.3186504\n",
            " -3.1033978  -3.7760084  -0.7202395  -2.475467   -0.98938084  2.0022655\n",
            "  1.9214784   2.1052954 ]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.4848, Accuracy: 15277/20000 (76.39%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.4283, Accuracy: 15681/20000 (78.41%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 1.0585, Accuracy: 10202/20000 (51.01%)\n",
            "\n",
            "Using penalty multiplier 80.0\n",
            "Train Epoch: 16 [0/20000 (0%)]\tERM loss: 0.906944\tGrad penalty: 0.000637\n",
            "First 20 logits [ 1.8254527  -3.9204895   1.5971423  -0.8097042   2.5832815  -0.559331\n",
            "  2.0095136  -2.2063615   2.6069298   2.3167796   0.6875122   2.430341\n",
            " -1.1896731  -1.0201662  -4.104567   -2.6723301   1.9673184  -2.7040088\n",
            " -0.79754734 -1.7597394 ]\n",
            "Train Epoch: 16 [4000/20000 (20%)]\tERM loss: 0.882637\tGrad penalty: 0.001355\n",
            "First 20 logits [-2.8997648   0.24335334  1.4821707  -4.5212383  -0.44111636  1.2066836\n",
            "  2.7861447  -0.46658513 -0.2498213   2.0137346   1.3831712   0.10631233\n",
            " -3.322497    2.135224   -0.4607521  -0.29634616 -0.5943393  -1.5898246\n",
            " -0.19138537  1.4584094 ]\n",
            "Train Epoch: 16 [8000/20000 (40%)]\tERM loss: 0.889004\tGrad penalty: 0.001937\n",
            "First 20 logits [ 1.2106726   1.4131353  -1.9805788   1.079668    2.0064478  -1.5491023\n",
            "  0.92826825  1.1123204  -2.2958648   1.0428003   0.01480258  0.94404775\n",
            "  2.1127193   2.1506388   2.2528682  -2.0641756   1.0459093   2.1166463\n",
            "  2.7647195  -0.8347629 ]\n",
            "Train Epoch: 16 [12000/20000 (60%)]\tERM loss: 0.969671\tGrad penalty: 0.002213\n",
            "First 20 logits [ 2.1385717   1.4188508  -0.46257454  1.0443193   1.0780121   0.2146697\n",
            " -1.2740488   1.3451803   2.5263975   1.1602602  -1.2361465   0.9247379\n",
            "  1.06221    -2.7144475   0.4049703   1.2754518  -3.142028    1.3946493\n",
            "  1.0247834  -0.60859895]\n",
            "Train Epoch: 16 [16000/20000 (80%)]\tERM loss: 0.915616\tGrad penalty: 0.001108\n",
            "First 20 logits [-0.30879712  1.5820283  -2.0136282   3.3646314   2.6519623   2.1862917\n",
            " -0.38371763  0.86604387 -1.8845612  -2.312681    2.3615973   0.46616226\n",
            "  1.5137998  -1.2020692  -1.8932943  -2.851271   -0.16134253 -0.16594768\n",
            "  2.0391247  -0.5568326 ]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.4625, Accuracy: 15430/20000 (77.15%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.4017, Accuracy: 15996/20000 (79.98%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 1.0287, Accuracy: 9441/20000 (47.20%)\n",
            "\n",
            "Using penalty multiplier 80.0\n",
            "Train Epoch: 17 [0/20000 (0%)]\tERM loss: 0.838881\tGrad penalty: 0.002455\n",
            "First 20 logits [ 2.048803    0.98810476  0.5450613  -2.7586136   3.125251   -0.1959309\n",
            "  0.3110119   3.1580672  -2.5741706  -2.0556262   1.2164798  -1.9018499\n",
            " -2.3270612   2.7659137   1.599077   -1.5757369   0.5623702   0.937212\n",
            " -2.725866   -0.3616387 ]\n",
            "Train Epoch: 17 [4000/20000 (20%)]\tERM loss: 0.825395\tGrad penalty: 0.001425\n",
            "First 20 logits [ 0.20307931 -2.7120373   0.19683394  3.1266375   0.9649017   2.2403355\n",
            " -3.4304252  -3.5000772  -3.232602    1.3249576   2.7747247  -2.5448158\n",
            " -1.8160186   2.7231092  -2.3452759   0.07045851 -3.015178    3.0033138\n",
            "  0.9929362   1.2173513 ]\n",
            "Train Epoch: 17 [8000/20000 (40%)]\tERM loss: 0.871634\tGrad penalty: 0.001156\n",
            "First 20 logits [-2.2207353  -0.45396465 -3.6665065   0.5335837  -0.5069227  -2.5206997\n",
            "  1.3266177  -1.2930875   2.7681875   1.4463872   1.7638909  -2.376637\n",
            "  1.096574    2.6135733   0.15292409 -1.1003983   2.4696565  -1.4657562\n",
            "  2.5083554   1.8707412 ]\n",
            "Train Epoch: 17 [12000/20000 (60%)]\tERM loss: 0.935726\tGrad penalty: 0.001003\n",
            "First 20 logits [-1.4137652   0.10166441 -0.21017335 -1.9392378  -0.4704933  -0.5974107\n",
            " -0.75800776 -2.2954476   0.7679722   1.4883957  -2.9115179   0.38994557\n",
            " -0.53843313  1.1530439  -2.6550395  -0.5275894   0.7340069  -0.21056853\n",
            " -0.7346397   1.0301268 ]\n",
            "Train Epoch: 17 [16000/20000 (80%)]\tERM loss: 0.995849\tGrad penalty: 0.000528\n",
            "First 20 logits [ 0.7615743  -0.9312859  -2.547383   -3.223261   -2.5869179   0.60759586\n",
            "  1.6917987   0.46199882  1.389673    0.21159907 -1.0218972  -2.8786838\n",
            "  2.6625485  -2.1837592  -2.90087    -1.3874172   2.0616415  -2.5708349\n",
            "  0.8651306  -2.7264504 ]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.5016, Accuracy: 14970/20000 (74.85%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.4711, Accuracy: 15086/20000 (75.43%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 0.8690, Accuracy: 11751/20000 (58.76%)\n",
            "\n",
            "Using penalty multiplier 90.0\n",
            "Train Epoch: 18 [0/20000 (0%)]\tERM loss: 0.949963\tGrad penalty: 0.002164\n",
            "First 20 logits [-1.5007398  -1.0279261  -1.2089952  -0.4901611  -0.03184471  1.3187752\n",
            " -1.864394    0.42044148 -1.6677176  -3.105118   -1.7315431   0.24065146\n",
            "  1.829583    1.9662867  -0.8661222   0.5295878  -0.48310104 -1.0020093\n",
            "  1.3898963  -1.5491894 ]\n",
            "Train Epoch: 18 [4000/20000 (20%)]\tERM loss: 0.955086\tGrad penalty: 0.000007\n",
            "First 20 logits [ 0.8951714  -0.07610545 -0.50729066  0.03206687 -1.5354315   0.54147923\n",
            "  1.6490128   1.3938051  -0.6569973  -3.427179   -1.9205     -2.1456182\n",
            " -1.0723312   0.8086973   1.938349   -2.5805535  -1.4014292  -1.5174384\n",
            "  1.1247548  -1.3708329 ]\n",
            "Train Epoch: 18 [8000/20000 (40%)]\tERM loss: 0.938276\tGrad penalty: 0.000647\n",
            "First 20 logits [-2.8538055  -0.90233546 -0.9500698   0.05279232 -0.4918956  -1.0273486\n",
            " -2.3659816  -2.584179   -2.5561044  -3.096757    1.7522181  -0.19782832\n",
            "  0.48814937  0.49347356 -1.8437744  -3.133698   -1.8488365   2.7260866\n",
            " -0.5448885  -2.4928951 ]\n",
            "Train Epoch: 18 [12000/20000 (60%)]\tERM loss: 0.904124\tGrad penalty: 0.000165\n",
            "First 20 logits [-3.5505586  -2.057292    2.311062    0.00542003 -0.5493748   1.8077244\n",
            " -0.6903626   2.246804    1.2356551  -2.1535847  -2.3914118   2.4827294\n",
            "  0.43773225  2.9449258  -2.3400116   1.8625935  -1.4243594   1.0303115\n",
            " -0.37714964  0.15534817]\n",
            "Train Epoch: 18 [16000/20000 (80%)]\tERM loss: 0.881889\tGrad penalty: 0.000725\n",
            "First 20 logits [ 2.6400127   2.631086   -2.1730044   0.02596451 -2.6209202   1.9017972\n",
            "  3.0978281   3.1168356  -1.8667886   2.263494    0.65206516  0.22703412\n",
            " -1.5805601  -0.26487482 -2.191941    1.0816481   2.5285945  -0.09580978\n",
            " -0.04655666  2.4031358 ]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.4519, Accuracy: 15538/20000 (77.69%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.3839, Accuracy: 16213/20000 (81.06%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 1.0816, Accuracy: 8875/20000 (44.38%)\n",
            "\n",
            "Using penalty multiplier 90.0\n",
            "Train Epoch: 19 [0/20000 (0%)]\tERM loss: 0.818888\tGrad penalty: 0.003073\n",
            "First 20 logits [-0.31818372  0.45880386  0.67112076 -0.05636344  2.0866768  -0.86852485\n",
            "  2.4743614   2.099726    0.53576905  2.6011164  -2.685902    2.4323616\n",
            "  1.8671999  -1.6820816  -2.7743578  -0.19170162  1.3540089   3.031786\n",
            " -1.200908    2.5619447 ]\n",
            "Train Epoch: 19 [4000/20000 (20%)]\tERM loss: 0.831355\tGrad penalty: 0.000611\n",
            "First 20 logits [ 2.370245   -1.2626094   0.44072664 -2.847575    0.6582813   2.8319962\n",
            " -0.12035204  0.0551827   2.3857007  -3.2639992   0.58919924  0.73308665\n",
            " -0.84489435  1.6739652  -0.7143615  -0.2722509   0.9514051  -3.2698483\n",
            "  0.4946429  -0.56045014]\n",
            "Train Epoch: 19 [8000/20000 (40%)]\tERM loss: 0.863132\tGrad penalty: 0.000095\n",
            "First 20 logits [-1.9971645   3.6698036  -1.4057682   0.14536801  0.6178028  -3.4704838\n",
            " -1.7424735  -0.514597    0.6930719  -3.272236    0.81397074 -1.8270671\n",
            "  0.21436301 -0.95269006  2.2230105   1.2802132   2.5792274   2.1239355\n",
            " -3.5720923   1.3026314 ]\n",
            "Train Epoch: 19 [12000/20000 (60%)]\tERM loss: 0.854754\tGrad penalty: 0.001072\n",
            "First 20 logits [ 2.6703162   2.3600903  -0.8318783  -0.45360857  1.7103258  -0.2540435\n",
            " -0.35714665 -3.2208796   2.4352114  -0.07149394  2.786821   -1.8622649\n",
            " -0.10623232  2.752701    0.01734533  2.815817   -2.1611943  -0.11531103\n",
            "  2.792707    1.4028674 ]\n",
            "Train Epoch: 19 [16000/20000 (80%)]\tERM loss: 0.885850\tGrad penalty: -0.000583\n",
            "First 20 logits [-3.251423   -0.76627403 -2.5864391   2.3382757  -2.9810705  -0.35841164\n",
            " -0.97549087 -0.24423392  1.9853579  -2.1953962   1.216449    2.8501122\n",
            " -2.1436536  -2.900481   -0.71421975  1.4036806  -2.3146565  -0.0512284\n",
            " -3.6638837   2.6380537 ]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.4644, Accuracy: 15387/20000 (76.94%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.4070, Accuracy: 15816/20000 (79.08%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 1.0531, Accuracy: 10191/20000 (50.95%)\n",
            "\n",
            "Using penalty multiplier 100.0\n",
            "Train Epoch: 20 [0/20000 (0%)]\tERM loss: 0.874405\tGrad penalty: 0.000632\n",
            "First 20 logits [ 0.42379135 -0.6310738  -1.0835187  -2.7710662   1.7654281  -3.3780792\n",
            "  2.7642994  -4.483533    0.82198215 -2.1426792   1.0992033   0.80748487\n",
            "  0.2691485  -1.8396991   2.6138604   1.4331588  -3.2669961   0.64403254\n",
            "  1.7091397  -2.7714372 ]\n",
            "Train Epoch: 20 [4000/20000 (20%)]\tERM loss: 0.859275\tGrad penalty: 0.001055\n",
            "First 20 logits [ 2.3004465e+00  2.4758513e+00 -2.4596436e+00 -1.6721228e+00\n",
            " -2.7979356e-01 -2.1731435e-01 -6.7092502e-01 -3.0195227e+00\n",
            " -3.6896873e+00  6.1282104e-01  1.8899214e+00 -3.2174253e+00\n",
            " -2.6545992e+00 -2.5258808e+00 -2.1900420e+00  4.7815427e-02\n",
            " -2.9734867e+00 -3.4982772e+00  2.7678348e-03  2.3112822e+00]\n",
            "Train Epoch: 20 [8000/20000 (40%)]\tERM loss: 0.893363\tGrad penalty: 0.002234\n",
            "First 20 logits [-3.4777188  -2.715732    1.5657094  -3.0273144  -2.403639   -2.8562489\n",
            " -0.34546414 -3.0890133  -3.145865   -3.1875055  -3.6053164   1.3080597\n",
            "  2.0056756  -3.3215032  -2.9989429   3.217715    0.53383315 -3.004039\n",
            " -1.2683415  -2.321464  ]\n",
            "Train Epoch: 20 [12000/20000 (60%)]\tERM loss: 0.891289\tGrad penalty: 0.000002\n",
            "First 20 logits [ 1.5977616  -3.9318807   2.326299    1.7082133   1.4964682   1.3979564\n",
            " -0.4551259  -2.529038   -0.21672752  0.5252326  -1.9441454   1.7979312\n",
            "  0.04913024  0.8634357  -3.023488    0.25050953  2.385323    1.2778478\n",
            "  1.1654732  -0.22669223]\n",
            "Train Epoch: 20 [16000/20000 (80%)]\tERM loss: 0.932686\tGrad penalty: 0.004129\n",
            "First 20 logits [ 2.1384985   1.8036839   0.3481788   0.3494023  -0.75420785  1.5180387\n",
            " -3.1726394   1.5556142  -0.23965546 -0.4092288  -1.6965346  -0.17041193\n",
            " -2.9054449   1.3775818  -0.10976349 -3.333455   -0.66128516 -3.6223419\n",
            " -2.2857692  -4.1691465 ]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.4914, Accuracy: 15033/20000 (75.17%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.4600, Accuracy: 15201/20000 (76.00%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 0.8613, Accuracy: 11681/20000 (58.41%)\n",
            "\n",
            "Using penalty multiplier 100.0\n",
            "Train Epoch: 21 [0/20000 (0%)]\tERM loss: 0.958474\tGrad penalty: 0.000978\n",
            "First 20 logits [-0.50368243 -0.9358015  -0.6086346   0.547702   -1.5089766  -0.34551284\n",
            "  0.9432521  -1.2637486   1.9044901  -1.0877632   0.40590224  2.803947\n",
            " -1.9409993   0.36218452  1.4836067  -0.37904567  0.60817605  1.6959584\n",
            " -2.0515869   1.2136985 ]\n",
            "Train Epoch: 21 [4000/20000 (20%)]\tERM loss: 1.018532\tGrad penalty: 0.000478\n",
            "First 20 logits [ 2.2388947  -0.50675166  1.4005133  -0.8181121  -2.0644052  -0.15167767\n",
            "  1.0846312   2.1297708  -0.79037213 -1.5038414  -0.04497967  1.2091447\n",
            "  2.9158497   2.2678556  -0.35148245  1.9393125  -1.8530173   2.7313378\n",
            "  0.8207333  -1.420709  ]\n",
            "Train Epoch: 21 [8000/20000 (40%)]\tERM loss: 0.974426\tGrad penalty: 0.002123\n",
            "First 20 logits [ 0.6931391   0.31738394  0.9020869  -0.04310138  2.2799375   0.22449511\n",
            " -1.1265333   0.09491232  1.0087835  -1.6776179   1.0469458   2.0875902\n",
            "  0.92420244  2.1482353  -1.7257663   0.48621485  0.23321608  1.3132803\n",
            "  1.6475013   1.557088  ]\n",
            "Train Epoch: 21 [12000/20000 (60%)]\tERM loss: 0.974737\tGrad penalty: 0.000422\n",
            "First 20 logits [ 0.83433276  2.9537005   1.3999934   2.9300346  -0.82762027  3.0931182\n",
            "  3.304129    2.0466938  -0.7210366  -0.5244021  -1.4206191  -0.44514683\n",
            " -0.62231785  2.5091186   2.3209374  -1.6962465  -1.0767605   2.0499082\n",
            "  1.4804673   0.10202648]\n",
            "Train Epoch: 21 [16000/20000 (80%)]\tERM loss: 0.948318\tGrad penalty: -0.000051\n",
            "First 20 logits [ 0.51763874 -2.4011288   2.7605824   0.23782972  0.91819733  2.4413815\n",
            "  2.3027854   2.1263433   2.1327636  -0.7865358   1.4867641  -2.3987062\n",
            "  1.2324779   2.8626375   1.0076616   0.9356928   3.0123231  -0.8145849\n",
            "  2.405238    1.6106886 ]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.4638, Accuracy: 15341/20000 (76.70%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.4135, Accuracy: 15720/20000 (78.60%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 0.9823, Accuracy: 10446/20000 (52.23%)\n",
            "\n",
            "Using penalty multiplier 110.0\n",
            "Train Epoch: 22 [0/20000 (0%)]\tERM loss: 0.898824\tGrad penalty: 0.000321\n",
            "First 20 logits [-2.5175848   3.3002787  -0.05337488 -0.36857784  0.45480075  0.67537725\n",
            " -3.0616207   2.081665    1.3182933   2.9156299   2.711458    0.92567843\n",
            " -0.14117683 -1.3057479  -0.57568586 -3.2392197   0.6610763  -3.5132806\n",
            "  0.65289706  0.6236824 ]\n",
            "Train Epoch: 22 [4000/20000 (20%)]\tERM loss: 0.865611\tGrad penalty: 0.001375\n",
            "First 20 logits [-2.6049988   1.0665187  -3.516447    1.211097   -1.2887617  -1.1044227\n",
            "  2.2195058  -2.141182   -3.7087846   2.5392275   1.9598926   1.1277144\n",
            "  0.14211884  1.7759522  -1.6768146   1.061771   -3.0663803  -3.5375185\n",
            "  0.35177293 -3.5720663 ]\n",
            "Train Epoch: 22 [8000/20000 (40%)]\tERM loss: 0.890909\tGrad penalty: 0.001708\n",
            "First 20 logits [ 2.6986709   0.6020323   2.0321448  -3.296481   -2.2833216   1.7494667\n",
            " -1.6590014  -3.3847318  -0.6171617   1.5127277   0.19295841  2.5386784\n",
            " -1.2871015  -1.304539    0.5461326   2.4214618  -3.6583796  -1.1036966\n",
            "  2.3276174  -0.06460826]\n",
            "Train Epoch: 22 [12000/20000 (60%)]\tERM loss: 0.872618\tGrad penalty: 0.001200\n",
            "First 20 logits [ 2.1974056  -1.373524   -0.09109791 -1.9780287  -2.7421417  -2.9594493\n",
            " -0.01753474 -0.60147804 -0.8629746  -0.33724144 -3.0051892  -3.631533\n",
            "  2.0132556   0.34983048  1.2194394  -3.4228592   1.1043854   1.1591626\n",
            "  1.2600467  -3.069956  ]\n",
            "Train Epoch: 22 [16000/20000 (80%)]\tERM loss: 0.908542\tGrad penalty: -0.000419\n",
            "First 20 logits [-1.5364859  -0.46055573 -0.5718284   1.6678033   0.6800069  -3.3263562\n",
            "  2.1624072  -2.3689494  -0.87160033  2.567966   -2.3650281   2.5634634\n",
            " -3.3224454  -0.6433521   1.1189209  -0.4165829  -1.1346186  -2.1681492\n",
            "  0.12374774 -2.0810394 ]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.4835, Accuracy: 15116/20000 (75.58%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.4450, Accuracy: 15297/20000 (76.48%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 0.9489, Accuracy: 11500/20000 (57.50%)\n",
            "\n",
            "Using penalty multiplier 110.0\n",
            "Train Epoch: 23 [0/20000 (0%)]\tERM loss: 0.920669\tGrad penalty: 0.000621\n",
            "First 20 logits [-3.6755161   3.6839833  -0.0956172   1.1317216   3.114431   -3.9943335\n",
            " -1.0592227  -2.012425    1.7687117  -0.92695224  1.4451381   2.0539103\n",
            " -0.57167     0.9687407   2.4224234   2.728964   -1.2241588   2.7446003\n",
            "  0.11415876 -1.2116399 ]\n",
            "Train Epoch: 23 [4000/20000 (20%)]\tERM loss: 0.884908\tGrad penalty: 0.000882\n",
            "First 20 logits [ 1.0200368  -2.1181145   1.6515715  -2.480536   -1.3689506   0.07285395\n",
            " -1.838995   -1.639279    0.7586778  -1.344567    1.8587053   2.7273548\n",
            " -0.44418943 -0.53483355 -2.1305997  -0.70777875 -1.0433172   3.2492945\n",
            " -1.6835278  -1.5124041 ]\n",
            "Train Epoch: 23 [8000/20000 (40%)]\tERM loss: 0.893067\tGrad penalty: 0.001034\n",
            "First 20 logits [ 1.4615408  -1.9263144   1.4618206  -0.22959706 -3.099445    0.9673896\n",
            "  1.933277    2.7468755  -1.3213633  -0.16794099 -0.5217806  -1.4342083\n",
            " -0.8289269   0.8461336  -1.1070515   2.386629    1.0924897  -0.07894821\n",
            " -1.2957054  -0.11609526]\n",
            "Train Epoch: 23 [12000/20000 (60%)]\tERM loss: 0.892844\tGrad penalty: 0.000225\n",
            "First 20 logits [-0.67907447 -0.7459578  -2.1773374   3.6223583   0.04829082 -2.59477\n",
            "  2.3998563  -0.27676     1.7202127  -1.1096609  -0.4631847   3.558965\n",
            "  0.08212675  0.3311388   0.28769067  1.9711082  -2.4618726   1.029994\n",
            "  1.1387048   2.8389504 ]\n",
            "Train Epoch: 23 [16000/20000 (80%)]\tERM loss: 0.870835\tGrad penalty: -0.000026\n",
            "First 20 logits [ 2.885667    0.47426823  0.553538    2.3706846   2.4792554  -3.072531\n",
            "  1.8584346   2.6400697  -0.9712673  -0.3220619   0.13717955  0.51571035\n",
            " -1.6403235  -3.1585667   1.0472792   1.2627454   2.3265607   2.8843865\n",
            "  2.609382    3.1725905 ]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.4640, Accuracy: 15323/20000 (76.61%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.4134, Accuracy: 15705/20000 (78.53%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 0.9985, Accuracy: 10405/20000 (52.02%)\n",
            "\n",
            "Using penalty multiplier 120.0\n",
            "Train Epoch: 24 [0/20000 (0%)]\tERM loss: 0.869463\tGrad penalty: 0.000577\n",
            "First 20 logits [-3.6208029   0.15558007 -1.786633    0.6747579  -0.90930444  1.9956251\n",
            "  1.845288    0.85025966  1.0026824   2.8911197   1.0233133  -1.0821903\n",
            "  1.8285276  -0.11243124 -0.40676153 -0.01047178  0.20280178  1.9324598\n",
            " -0.73251456 -2.2177339 ]\n",
            "Train Epoch: 24 [4000/20000 (20%)]\tERM loss: 0.882512\tGrad penalty: 0.000653\n",
            "First 20 logits [ 1.7388623  -0.68828696 -1.3179646   1.0214922  -0.6700686  -0.23072386\n",
            "  0.8477993   1.7911582   0.25256222  2.610697    1.9097959   0.14441575\n",
            " -0.9482682  -0.27618814  0.40917805  2.632993   -0.68678325 -2.29206\n",
            " -3.3442323   2.0566099 ]\n",
            "Train Epoch: 24 [8000/20000 (40%)]\tERM loss: 0.865577\tGrad penalty: -0.000144\n",
            "First 20 logits [ 0.05936991 -1.3534889   1.3059012  -2.3066306  -2.1426501   1.7052788\n",
            " -2.376836    2.823149    0.4802882   0.06183651 -2.2756371  -0.17267478\n",
            " -1.4136859   0.70876664  1.8602314  -0.20626639  2.6352518  -1.0612903\n",
            " -2.8516073   0.16330905]\n",
            "Train Epoch: 24 [12000/20000 (60%)]\tERM loss: 0.915310\tGrad penalty: 0.000126\n",
            "First 20 logits [-1.9179527  -2.5107956  -2.2937095  -1.1879035  -1.5973812   3.4225354\n",
            " -2.3706112   3.1873968   1.0202146  -2.8799634   2.1425834   1.2916465\n",
            " -0.5916967  -2.0271628  -0.61021554 -0.03854721  3.1566224  -1.5091437\n",
            " -1.3454671  -1.4311336 ]\n",
            "Train Epoch: 24 [16000/20000 (80%)]\tERM loss: 0.914735\tGrad penalty: 0.000755\n",
            "First 20 logits [ 2.4173274   1.9757812   0.9940225   0.90380895 -0.04276382  1.3624542\n",
            "  2.3472624  -2.082282    0.85713434 -1.6618772  -2.079992    2.8570156\n",
            "  0.19993287 -0.59202206 -2.1293118   1.6087554  -2.4656188  -0.6455092\n",
            " -0.3339119  -2.2695022 ]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.4701, Accuracy: 15284/20000 (76.42%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.4276, Accuracy: 15528/20000 (77.64%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 0.9771, Accuracy: 10764/20000 (53.82%)\n",
            "\n",
            "Using penalty multiplier 120.0\n",
            "Train Epoch: 25 [0/20000 (0%)]\tERM loss: 0.902609\tGrad penalty: -0.000011\n",
            "First 20 logits [ 0.34103414  0.3866182  -1.5337073  -1.6792517  -2.7905734  -0.6196792\n",
            "  1.0277735   2.4052365   0.9276017  -1.9767506  -2.8248436   0.44846836\n",
            " -1.4149641  -0.42274958 -1.8865371  -2.0686133   1.6139578  -0.4752287\n",
            " -2.5761232   2.156314  ]\n",
            "Train Epoch: 25 [4000/20000 (20%)]\tERM loss: 0.899983\tGrad penalty: 0.000007\n",
            "First 20 logits [-1.2527554  -3.1380825  -0.51304805  2.1980567   0.6161979  -0.23779553\n",
            "  1.4252317   0.8609911  -1.8412368   0.24051431 -1.2686677   2.5016303\n",
            " -2.8857698  -0.15145996  2.208251   -4.2740626   0.01702821  1.1895268\n",
            "  1.0067234   2.100543  ]\n",
            "Train Epoch: 25 [8000/20000 (40%)]\tERM loss: 0.882494\tGrad penalty: 0.000845\n",
            "First 20 logits [-3.2202268  -2.3003225  -1.6374152  -0.30372703  0.23763846 -2.13607\n",
            " -1.3149807  -1.4251744  -2.1835382   0.1677829  -0.37607417  1.0666335\n",
            " -0.6420536   1.0231185   1.4569935   2.2375858  -2.6472976  -1.6471488\n",
            " -0.25217757 -1.042525  ]\n",
            "Train Epoch: 25 [12000/20000 (60%)]\tERM loss: 0.893351\tGrad penalty: 0.001647\n",
            "First 20 logits [ 0.3763749  -0.58123374 -1.3773285  -3.0425992   1.0273107   0.584458\n",
            " -2.7221136   1.7979087   2.0237825   2.2596505   0.21010156  0.521225\n",
            "  0.8453819  -1.6098146  -2.525009   -1.1011686  -2.532393   -1.7225708\n",
            " -0.76552045  0.56467456]\n",
            "Train Epoch: 25 [16000/20000 (80%)]\tERM loss: 0.903034\tGrad penalty: 0.000188\n",
            "First 20 logits [ 0.10474437  0.60487944 -1.3303804   1.4985814  -2.4723947  -0.19222501\n",
            " -3.5824158   2.6195729  -4.045591    2.6585214  -2.0930083   2.1740363\n",
            " -0.16502869  2.6730795  -1.6435082   0.44250926 -0.9443708   0.7293247\n",
            "  2.098351    2.951221  ]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.4673, Accuracy: 15304/20000 (76.52%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.4201, Accuracy: 15673/20000 (78.36%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 1.0078, Accuracy: 10403/20000 (52.02%)\n",
            "\n",
            "Using penalty multiplier 130.0\n",
            "Train Epoch: 26 [0/20000 (0%)]\tERM loss: 0.934323\tGrad penalty: 0.002001\n",
            "First 20 logits [-0.22567111 -0.539977    1.7160981  -2.9267876   0.09618567  1.5446169\n",
            "  1.2049257   0.2039386   0.6435708   2.697436    3.086853   -2.5335007\n",
            " -3.0367293   2.2160094   0.6583675   0.13374202  2.6561341  -2.1209757\n",
            "  1.333134    2.3511698 ]\n",
            "Train Epoch: 26 [4000/20000 (20%)]\tERM loss: 0.865159\tGrad penalty: 0.001624\n",
            "First 20 logits [-2.198365    1.6106284  -0.4115622  -0.4332207   1.055574   -2.8043418\n",
            " -3.23518     1.9729338   0.59858865 -1.7194382   2.0074213  -1.3826557\n",
            " -2.0058644  -0.63355124  0.8964214   0.42649725 -2.227854   -3.1926894\n",
            " -2.9578385   1.8675492 ]\n",
            "Train Epoch: 26 [8000/20000 (40%)]\tERM loss: 0.867513\tGrad penalty: 0.002266\n",
            "First 20 logits [ 3.6215076  -2.3894014  -3.3114755   2.0553613   0.4055782  -0.3950482\n",
            "  0.03063996 -2.8581467  -2.8891249  -0.39992657  0.5696134  -2.777757\n",
            " -0.2713877   1.1028272  -3.0143285   2.0674744  -0.26653412 -1.5121483\n",
            " -1.3126105  -1.2487438 ]\n",
            "Train Epoch: 26 [12000/20000 (60%)]\tERM loss: 0.895282\tGrad penalty: -0.000178\n",
            "First 20 logits [-0.05765021 -2.4992192   1.8514084  -1.3529246  -3.1624036   1.8765748\n",
            "  2.9524114  -3.362728   -2.7953672   1.818728    0.7858053  -0.05622777\n",
            "  0.8488019  -1.0772519  -3.5073242   1.5987543   1.2230563  -1.0592924\n",
            "  2.8793788   0.20103377]\n",
            "Train Epoch: 26 [16000/20000 (80%)]\tERM loss: 0.971278\tGrad penalty: -0.000408\n",
            "First 20 logits [ 1.238519   -1.788873    2.044469   -2.779092   -2.4016173  -1.2189752\n",
            " -2.7369342  -0.20520039  2.7583637   2.1615145  -2.9931042   1.6595538\n",
            " -1.4321207   1.1950482   0.6289574  -0.271217    0.7860454   0.63348424\n",
            " -0.2945597   0.74003774]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.4854, Accuracy: 15090/20000 (75.45%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.4590, Accuracy: 15151/20000 (75.75%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 0.8723, Accuracy: 11859/20000 (59.30%)\n",
            "\n",
            "Using penalty multiplier 130.0\n",
            "Train Epoch: 27 [0/20000 (0%)]\tERM loss: 0.949251\tGrad penalty: 0.000043\n",
            "First 20 logits [-2.2637572  -1.4295223  -0.4604802   2.6016605  -1.8370218   1.5833805\n",
            "  2.4937572   0.7732832  -2.011066   -2.8597822   1.6685575  -2.5535035\n",
            " -0.44370914  0.73256314  1.4113828   1.0554534   1.1116632   1.2386104\n",
            " -1.3050607   2.1049094 ]\n",
            "Train Epoch: 27 [4000/20000 (20%)]\tERM loss: 0.937106\tGrad penalty: 0.000117\n",
            "First 20 logits [-2.9442308  -0.6745666   2.3327048   2.911587    0.9102455  -3.545647\n",
            "  1.495471    1.8312547  -1.0283175  -0.77450943 -2.2646585   1.5449607\n",
            "  0.7280079   1.97174     1.3342838   1.3885632  -0.7852676  -1.5811497\n",
            " -1.6938425  -0.8311653 ]\n",
            "Train Epoch: 27 [8000/20000 (40%)]\tERM loss: 0.921054\tGrad penalty: 0.000384\n",
            "First 20 logits [-1.9224923  -2.7517946   3.4896827   2.1602364  -1.5772766  -1.6891533\n",
            " -0.5805181   0.6494292  -0.68787277  1.5738437   1.0362338  -0.19619161\n",
            "  0.96494746 -3.1456974  -3.128573   -2.0612168   2.6715868   2.2790334\n",
            "  1.1561649   2.1990726 ]\n",
            "Train Epoch: 27 [12000/20000 (60%)]\tERM loss: 0.870790\tGrad penalty: -0.000108\n",
            "First 20 logits [-0.40099016 -2.4675217   1.9204543  -0.23248285  1.1196607   0.7511571\n",
            "  2.002069    1.3342937   2.478199   -2.244607   -3.1943347  -0.17037562\n",
            " -2.3183193   2.2381902   0.08085389 -1.3768178  -2.5996144   1.7001233\n",
            "  1.1797076   1.75344   ]\n",
            "Train Epoch: 27 [16000/20000 (80%)]\tERM loss: 0.824354\tGrad penalty: 0.002330\n",
            "First 20 logits [-2.4528394   2.424621   -1.2431419   0.9599919  -0.90349394  2.2330706\n",
            " -3.3551164  -3.9786088  -2.0468547  -3.4995224   2.091307   -0.24294095\n",
            " -1.6663513   0.02230925  2.285237   -0.24916236  0.69302744 -4.0379944\n",
            "  2.6019523   0.01028544]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.4552, Accuracy: 15497/20000 (77.48%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.4050, Accuracy: 15748/20000 (78.74%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 1.0413, Accuracy: 10214/20000 (51.07%)\n",
            "\n",
            "Using penalty multiplier 140.0\n",
            "Train Epoch: 28 [0/20000 (0%)]\tERM loss: 0.851940\tGrad penalty: 0.000054\n",
            "First 20 logits [-0.7256982   2.644595   -2.573821   -0.19433548  2.0964515  -2.4936335\n",
            " -3.12009    -2.8307657   1.4439865   0.82949364 -3.481198   -1.8247461\n",
            " -0.60237586  2.035736    1.4210665   0.68233836 -2.904375    2.3627863\n",
            "  0.42659053 -2.7261992 ]\n",
            "Train Epoch: 28 [4000/20000 (20%)]\tERM loss: 0.914233\tGrad penalty: 0.000485\n",
            "First 20 logits [-2.8154676   0.36843312 -1.588872    3.2303555  -0.6194251  -0.28717875\n",
            "  2.051354    1.2793045  -0.16461644  2.6236255   0.40467212  0.24158983\n",
            "  0.43357396  0.7638006  -1.2584937   1.8960556   1.8362947  -3.1094184\n",
            "  2.679529   -2.4129066 ]\n",
            "Train Epoch: 28 [8000/20000 (40%)]\tERM loss: 0.922091\tGrad penalty: 0.000185\n",
            "First 20 logits [ 1.4179301   1.7007403   1.6131606   1.164184    0.3902055   2.4866862\n",
            " -0.26561773 -1.5692558   0.3221838  -1.2287489   2.3469238   3.1418736\n",
            " -0.29940617  2.414945    1.2259107   1.0407968   1.1719514   2.4489093\n",
            " -0.36227563 -1.2010846 ]\n",
            "Train Epoch: 28 [12000/20000 (60%)]\tERM loss: 0.977754\tGrad penalty: 0.000053\n",
            "First 20 logits [-0.21697126  2.1702511  -1.1285757  -2.3773887   0.9733036  -2.5159132\n",
            "  2.2469158  -1.2449471   2.8300846  -2.4463754  -0.84535974  2.6657805\n",
            " -0.24755515  3.0036561  -1.3067727   1.7664148   2.5995286   1.2805355\n",
            " -2.1924233  -1.9094938 ]\n",
            "Train Epoch: 28 [16000/20000 (80%)]\tERM loss: 0.973999\tGrad penalty: 0.000102\n",
            "First 20 logits [-2.8510864  -1.9603792  -2.287033   -1.4453979  -2.4508579  -2.179453\n",
            " -0.8580941   0.42583418  2.535051   -0.81216496  0.8272548   1.9811312\n",
            "  2.3440948  -2.6809378   0.50035703 -0.7474858   1.9857358  -2.1640167\n",
            "  1.2476599   2.2858202 ]\n",
            "testing on train1 set\n",
            "\n",
            "Test set: Average loss: 0.4860, Accuracy: 15054/20000 (75.27%)\n",
            "\n",
            "testing on train2 set\n",
            "\n",
            "Test set: Average loss: 0.4597, Accuracy: 15090/20000 (75.45%)\n",
            "\n",
            "testing on test set\n",
            "\n",
            "Test set: Average loss: 0.8811, Accuracy: 12131/20000 (60.66%)\n",
            "\n",
            "Using penalty multiplier 140.0\n",
            "Train Epoch: 29 [0/20000 (0%)]\tERM loss: 0.926925\tGrad penalty: 0.000019\n",
            "First 20 logits [ 1.6503531  -3.0878813  -0.11003043  0.65868914  2.4558463   3.4739146\n",
            "  2.6522574  -2.347967   -0.92703235  1.508626   -1.93272    -0.48002973\n",
            " -0.52275014  2.0112362  -1.5213907   0.79826677  0.9767407  -2.9137247\n",
            "  0.7079698  -0.78603625]\n",
            "Train Epoch: 29 [4000/20000 (20%)]\tERM loss: 0.924379\tGrad penalty: 0.001390\n",
            "First 20 logits [ 0.14514638 -0.5904608   2.7071438   2.164657   -0.15087105  1.3296666\n",
            " -1.7741007   1.6378785  -0.29734084  0.01050932  1.5492344  -0.5357335\n",
            " -1.64098     2.344143    2.4007568   0.20015089 -2.036784    0.12050714\n",
            " -1.2162323  -3.000833  ]\n",
            "Train Epoch: 29 [8000/20000 (40%)]\tERM loss: 0.860961\tGrad penalty: 0.001939\n",
            "First 20 logits [-2.7653353   2.1746223   2.2689784   3.078842   -2.24999     0.71136415\n",
            " -2.9261851   0.1690372   2.7186797  -2.4293268  -1.041193    2.180353\n",
            "  4.0892954   2.155648    0.93105924 -0.3633652   0.19484007  2.5112262\n",
            " -1.4948149   1.8630201 ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-984e91dd3e17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_and_test_irm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-ffd32deb4648>\u001b[0m in \u001b[0;36mtrain_and_test_irm\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mirm_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain1_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain2_loader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'testing on train1 set'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain1_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-ffd32deb4648>\u001b[0m in \u001b[0;36mirm_train\u001b[0;34m(model, device, train_loaders, optimizer, epoch)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mpenalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loaders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m       \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_batch\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGh00cHNxpPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}